{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d9f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13dbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f20dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load ./DA_fm_analyze/scripts/tesseract/images/d1.JPG\n",
      "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Could not load ./DA_fm_analyze/scripts/tesseract/images/d2.JPG\n",
      "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Could not load ./DA_fm_analyze/.git/index\n",
      "'utf-8' codec can't decode byte 0xe5 in position 17: invalid continuation byte\n",
      "Could not load ./DA_fm_analyze/.git/objects/pack/pack-f798eee2d00d3b196f8760341e72b1efde610a92.idx\n",
      "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "Could not load ./DA_fm_analyze/.git/objects/pack/pack-f798eee2d00d3b196f8760341e72b1efde610a92.pack\n",
      "'utf-8' codec can't decode byte 0x91 in position 11: invalid start byte\n",
      "Could not load ./DA_fm_analyze/fmf/Important_Attrs_Pos.fmf\n",
      "'utf-8' codec can't decode byte 0x81 in position 9: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = './DA_fm_analyze'\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "        except Exception as e: \n",
    "            print(f\"Could not load {dirpath}/{file}\")\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10f8393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import argparse\\nimport os\\n\\nfrom fmanalyze.attrs.instructions import *\\nfrom fmanalyze.aggregate.main import create_dfs_for_basedir\\n\\npd.options.mode.chained_assignment = None\\nimport yaml\\nimport shutil\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\', help=\\'Path to config file\\', default=None, required=False)\\n    args = parser.parse_args()\\n    with open(args.config, \\'r\\') as confhandle:\\n        config = yaml.safe_load(confhandle)\\n\\n    sourcedir = config.get(\"source_dir\", None)\\n    targetdir = config.get(\"target_dir\", None)\\n\\n    if sourcedir is not None and targetdir is not None:\\n        teams_dir = os.path.join(targetdir, \\'teams\\')\\n\\n        for rtf_filame in os.listdir(sourcedir):\\n            print(f\\'Processing {rtf_filame}...\\')\\n            dir_to_create = os.path.splitext(rtf_filame)[0]\\n            basedir = os.path.join(teams_dir, dir_to_create)\\n\\n            os.makedirs(os.path.join(basedir), exist_ok=True)\\n            shutil.copyfile(os.path.join(sourcedir, rtf_filame), os.path.join(basedir, rtf_filame))\\n            print(f\\'Creating dfs for {basedir}...\\')\\n\\n    if targetdir is not None:\\n        print(\"Creating dfs for all dirs in targetdir...\")\\n        for dir in os.listdir(teams_dir):\\n            basedir = os.path.join(teams_dir, dir)\\n            create_dfs_for_basedir(basedir)', metadata={'source': './DA_fm_analyze/copy_reports.py'}),\n",
       " Document(page_content='import argparse\\nimport pandas as pd\\nimport os\\nfrom fmanalyze.roles.extract import extract_match_roles\\n\\npd.options.mode.chained_assignment = None\\nimport yaml\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\', help=\\'Path to config file\\', default=None, required=False)\\n    args = parser.parse_args()\\n    with open(args.config, \\'r\\') as confhandle:\\n        config = yaml.safe_load(confhandle)\\n    basedir = config[\"basedir\"]\\n    team = config.get(\"team\", None)\\n    if team:\\n        teams_dir = os.path.join(basedir, \\'teams\\', team)\\n        full_df = extract_match_roles(teams_dir)\\n    else:\\n        for team in os.listdir(os.path.join(basedir, \\'teams\\')):\\n            teams_dir = os.path.join(basedir, \\'teams\\', team)\\n            full_df = extract_match_roles(teams_dir)', metadata={'source': './DA_fm_analyze/position_report.py'}),\n",
       " Document(page_content='import argparse\\n\\nfrom fmanalyze.attrs.instructions import *\\nfrom fmanalyze.stats.leagues import generate_all_combinations\\nimport os\\npd.options.mode.chained_assignment = None\\nimport yaml\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\', help=\\'Path to config file\\', default=None, required=False)\\n    args = parser.parse_args()\\n    with open(args.config, \\'r\\') as confhandle:\\n        config = yaml.safe_load(confhandle)\\n\\n    sourcedir = config.get(\"source_dir\", None)\\n    targetdir = config.get(\"target_dir\", None)\\n    if sourcedir is not None and targetdir is not None:\\n\\n        roledir = os.path.join(targetdir, \\'roles\\')\\n        teamdir = os.path.join(targetdir, \\'teams\\')\\n        os.makedirs(roledir, exist_ok=True)\\n        generate_all_combinations(roledir, teamdir)', metadata={'source': './DA_fm_analyze/combine_dfs.py'}),\n",
       " Document(page_content='# FOOTBALL MANAGER ANALYZERS\\n\\n## COMPANION DATA PROJECT   \\nTo check the data structure see the companion project [here](https://github.com/diegoami/DA_fm_data)\\n\\n\\n## IMPORT DATA\\n\\n1. Import the view `fmf/Important_Attrs_Pos.fmf` into Football Manager\\n2. Export the stats of your team and your team into your league\\n3. Set up a yaml file, for instance like the one in `yamls/laliga.yml`, including as the `sourcedir` the location of where you saved your stats files, and as the `targetdir` the location where you want to set up the directory structure for the teams analysis. We will use this file as an example.\\n4. Run `python3 copy_reports.py -c leagues/laliga.yml` to generate the analysis for all teams. They will be in the `teams` subdirectory of the `targetdir` you specified in the yaml file\\n5. Run `python3 combine_dfs.py -c leagues/laliga.yml -t` to generate dataset of all players that can play in a roles, that will be put into the `roles` subdirectory of the `targetdir` you specified in the yaml file\\n6. Run `python3 save_quantiles.py -c leagues/laliga.yml -p` to generate the quantiles for all the attributes, that will be put into the `quantiles` subdirectory of the `targetdir` you specified in the yaml file\\n\\n## AVAILABLE APPLICATIONS\\n\\nThere are two application that you can start to analyze your squad and the squad of your rival team\\n\\n* `view_squad.py` to analyze your squad\\n* `view_formation.py` to analyze a formation\\n\\nThe purpose of each application is to select a list of players and show how they compare to the rest of the league in all attributes. This is done using a color schema highlighting the quantiles of each attribute.\\n\\n## VIEWING YOUR SQUAD\\n\\n1. Create a yaml file with the name of the team you want to analyze, as an example check `squads/rsana.yml`\\n2. Find the `full_squad.csv` file of your team that will be in the `teams` subdirectory of the `targetdir` you specified in the yaml file. It will contain all possible position for your squad, but all lines are commented out\\n3. (Optional) Make a copy of the `full_squad.csv` file and name it for instance `formation.csv`. Define in the yaml file the `formation` attribute as the name to this file. \\n4. Run `python3 view_squad.py -c squads/rsana.yml` to visualize the analysis for your team - select the `squad` button to see the attributes of your squad or formation\\n5. To filter your squad by position, select the `config` button and select the position you want to filter by. Use the back button to pick another position.\\n6. The `squad` button from the landing page will ead directly to your squad list, possibly filtered\\n\\n## VIEWING A FORMATION\\n\\n1. Create a yaml file containing the name of the teams whose formations you want to analyze, as an example `formations/rsana.yml`\\n2. Optionally, define and create a formation file containing a subset of the squad, and define the name of the file as `load_formation` and `load_rival_formation` in the yaml file. \\n3. Run `python3 view_formation.py -c formations/rsana.yml` to visualize the analysis for your team - select the `squad` button to see the attributes of your squad or formation\\n4. Use the `config` button to select the players you want to analyze - the formation will be saved to the files you defined as `save_formation` and `save_rival_formation` in the yaml file\\n5. In the `formation` page, you can see the attributes of the formations yous selected', metadata={'source': './DA_fm_analyze/README.md'}),\n",
       " Document(page_content='import argparse\\nimport os\\nimport yaml\\n\\nfrom fmanalyze.stats.quantiles import save_stats_for_attrs\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\')\\n    args = parser.parse_args()\\n    if args.config == None:\\n        print(\"required argument --config <config>\")\\n        exit()\\n    else:\\n        with open(args.config, \\'r\\') as confhandle:\\n            config = yaml.safe_load(confhandle)\\n\\n    targetdir = config.get(\"target_dir\", None)\\n    if targetdir is not None:\\n        rolesdir = os.path.join(targetdir, \\'roles\\')\\n        quantilesdir = os.path.join(targetdir, \\'quantiles\\')\\n        save_stats_for_attrs(rolesdir, quantilesdir, \\'attrs\\')\\n        save_stats_for_attrs(rolesdir, quantilesdir, \\'octs\\')\\n        save_stats_for_attrs(rolesdir, quantilesdir, \\'gk_octs\\')\\n\\n        save_stats_for_attrs(rolesdir, quantilesdir, \\'abis\\')', metadata={'source': './DA_fm_analyze/save_quantiles.py'}),\n",
       " Document(page_content='import dash\\nfrom dash import html\\nfrom dash import dcc\\nimport argparse\\nimport yaml\\nimport pandas as pd\\nfrom fmanalyze.aggregate.collect import create_formation_dfs, read_formations\\nimport os\\nfrom dash.dependencies import State\\nfrom fmanalyze.roles.formation import read_formation_for_select, read_selected_formation\\nfrom fmanalyze.ui.dash_helper import fill_style_conditions, create_fm_data_table, create_formation_layout\\nfrom dash.dependencies import Input, Output\\nimport dash_html_components as html\\nfrom flask import Flask, render_template\\n\\n\\nconfig = None\\n# Create a Flask app\\nserver = Flask(__name__)\\n\\n\\napp_formations = dash.Dash(__name__, server=server, url_base_pathname=\\'/formations/\\')\\napp_config = dash.Dash(__name__, server=server, url_base_pathname=\\'/config/\\')\\n\\n\\n\\nown_all_dfs = {}\\nrival_all_dfs = {}\\ncolor_dfs = {}\\nrival_color_dfs = {}\\ntab_dfs = {\"OCTAGON\" : [\\'octs\\', \\'gk_octs\\'],\\n           \"ATTRIBUTES\" : [\\'tec\\', \\'men\\', \\'phys\\', \\'goalk\\'],\\n           \"ABILITIES\" : [\\'tecabi\\', \\'menabi\\', \\'physabi\\']}\\n\\n@server.route(\\'/\\')\\ndef index():\\n    return render_template(\\'formation_index.html\\')\\n\\nnum_comboboxes = 44\\ncombobox_names = [f\\'own-role{index}-dropdown\\' for index in range(1,12)] + [f\\'own-player{index}-dropdown\\' for index in range(1,12)] + [f\\'rival-role{index}-dropdown\\' for index in range(1,12)] + [f\\'rival-player{index}-dropdown\\' for index in range(1,12)]\\n\\n@app_config.callback(\\n    Output(\\'redirect\\', \\'href\\'),\\n    Output(\\'redirect\\', \\'children\\'),\\n    [Input(\\'submit-button\\', \\'n_clicks\\')],\\n    [State(combobox_name, \\'value\\') for combobox_name in combobox_names]\\n)\\ndef on_button_click(n_clicks, *args):\\n    if int(n_clicks) > 0:  # Cast n_clicks to an integer\\n\\n        own_formation = create_formation_df(args[:11], args[11:22])\\n        rival_formation = create_formation_df(args[22:33], args[33:])\\n        own_formation.dropna(inplace=True)\\n        rival_formation.dropna(inplace=True)\\n        reload(own_formation, rival_formation)\\n        return \\'/formations\\', \\'Open Formations\\'\\n    return dash.no_update, dash.no_update\\n\\n\\ndef create_formation_df(positions, uids):\\n    df = pd.DataFrame(columns=[\\'Position\\', \\'UID\\'])\\n    df[\\'Position\\'] = positions\\n    df[\\'UID\\'] = uids\\n    df.dropna(inplace=True)\\n    df[\\'UID\\'] = df[\\'UID\\'].astype(\\'int64\\')\\n    return df\\n\\ndef process_formation():\\n    pass\\n\\n@app_formations.callback(Output(\\'tab-content\\', \\'children\\'), [Input(\\'tabs\\', \\'value\\')])\\ndef render_content(tab):\\n    table_names = tab_dfs[tab]\\n\\n    color_tables = [color_dfs[f\\'{table_name}_color\\'].drop(columns=[\\'UID\\']) for table_name in table_names]\\n    df_tables = [own_all_dfs[table_name].drop(columns=[\\'UID\\']) for table_name in table_names]\\n    style_condition_tables = [fill_style_conditions(color_table) for color_table in color_tables]\\n    # Set a fixed width for each column in pixels\\n\\n\\n    # Create the DataTable for df\\n    table_dfs = [create_fm_data_table(df_table, style_condition_table) for df_table, style_condition_table in zip(df_tables, style_condition_tables)]\\n\\n\\n    if rival_all_dfs:\\n        rival_df_tables = [rival_all_dfs[table_name].drop(columns=[\\'UID\\']) for table_name in table_names]\\n        rival_color_tables =[rival_color_dfs[f\\'{table_name}_color\\'].drop(columns=[\\'UID\\']) for table_name in table_names]\\n        rival_style_condition_tables = [fill_style_conditions(rival_color_table) for rival_color_table in rival_color_tables]', metadata={'source': './DA_fm_analyze/view_formation.py'}),\n",
       " Document(page_content='table_rival_dfs = [create_fm_data_table(rival_df_table, rival_style_condition_table) for rival_df_table, rival_style_condition_table in zip(rival_df_tables, rival_style_condition_tables)]\\n        return html.Div([\\n            html.Div([\\n                html.H4(\\'Own Data\\'),\\n                html.Div([\\n                    *table_dfs\\n                ], className=\\'tables-container\\')\\n            ]),\\n            html.Div([\\n                html.H4(\\'Rival Data\\'),\\n                html.Div([\\n                    *table_rival_dfs\\n                ], className=\\'tables-container\\')\\n            ])\\n        ])\\n    else:\\n        return html.Div([\\n            html.H4(\\'Own Data\\'),\\n            html.Div([\\n                *table_dfs\\n            ], className=\\'tables-container\\')\\n        ])\\n\\n\\ndef reload(own_formation = None, rival_formation = None):\\n    basedir, teamname, rivalname = config[\"basedir\"], config[\"team\"], config.get(\"rival\", None)\\n    teamdir, rivaldir = os.path.join(basedir, \\'teams\\', teamname), os.path.join(basedir, \\'teams\\',\\n                                                                               rivalname) if rivalname else None\\n    quantilesdir = os.path.join(basedir, \\'quantiles\\')\\n    load_formation, load_rival_formation = config.get(\"load_formation\", None), config.get(\"load_rival_formation\", None)\\n    save_formation, save_rival_formation = config.get(\"save_formation\", \\'formation_current.csv\\'), config.get(\\'save_rival_formation\\', \\'formation_current.csv\\')\\n    if own_formation is not None and rival_formation is not None:\\n        formation_df, formation_rival_df = own_formation, rival_formation\\n        own_formation.to_csv(os.path.join(teamdir, save_formation), index=False)\\n        rival_formation.to_csv(os.path.join(rivaldir, save_rival_formation), index=False)\\n    else:\\n        formation_df, formation_rival_df = read_formations(teamdir, load_formation, rivaldir, load_rival_formation)\\n\\n    create_formation_dfs(teamdir, rivaldir, quantilesdir, formation_df, formation_rival_df,\\n                         own_all_dfs, color_dfs, rival_all_dfs, rival_color_dfs)\\n    # Define the layout of the app\\n    app_formations.layout = create_formation_layout(tab_dfs, \\'OCTAGON\\')\\n\\n\\ndef create_config_layout():\\n    basedir, teamname, rivalname = config[\"basedir\"], config[\"team\"], config.get(\"rival\", None)\\n    teamdir, rivaldir = os.path.join(basedir, \\'teams\\', teamname), os.path.join(basedir, \\'teams\\',\\n                                                                               rivalname) if rivalname else None\\n\\n    load_formation, load_rival_formation = config.get(\"load_formation\", None), config.get(\"load_rival_formation\", None)\\n    save_formation, save_rival_formation = config.get(\"save_formation\", None), config.get(\"save_rival_formation\", None)\\n\\n    team_dict = read_formation_for_select(teamdir, \\'full_squad.csv\\')\\n    rival_dict = read_formation_for_select(rivaldir, \\'full_squad.csv\\')\\n    if load_formation:\\n        formation_lists = read_selected_formation(teamdir, load_formation)\\n    if load_rival_formation:\\n        rival_formation_lists = read_selected_formation(rivaldir, load_rival_formation)\\n    columns = create_player_columns(team_dict, \\'own\\', formation_lists)\\n    rival_columns = create_player_columns(rival_dict, \\'rival\\', rival_formation_lists)\\n\\n\\n\\n    return html.Div([\\n        html.H1(\\'Config\\'),\\n        html.H2(f\\'Loaded from {load_formation},  saving to  {save_formation}\\'),\\n        html.Div(columns, className=\\'row\\'),\\n        html.H1(\\'Rival Config\\'),\\n        html.H2(f\\'Loaded from {load_rival_formation}, saving to  {save_rival_formation}\\'),\\n        html.Div(rival_columns, className=\\'row\\'),\\n        html.Button(\\'Submit\\', id=\\'submit-button\\', n_clicks=0),\\n        html.A(\\'\\', id=\\'redirect\\', target=\\'_blank\\')\\n    ])', metadata={'source': './DA_fm_analyze/view_formation.py'}),\n",
       " Document(page_content='def create_player_columns(team_dict, prefix=\\'own\\', formation_lists=None):\\n    if formation_lists:\\n        role_dropdowns_defaults, player_dropdowns_defaults = formation_lists\\n    columns = []\\n    for index in range(1, 12):\\n        columns.append(html.Div([\\n            html.Label(f\\'Role {index}\\'),\\n            dcc.Dropdown(\\n                id=f\\'{prefix}-role{index}-dropdown\\',\\n                options=[{\\'label\\': value, \\'value\\': value} for value in\\n                         [\\'GK\\', \\'DR\\', \\'DC\\', \\'DL\\', \\'WBR\\', \\'DM\\', \\'WBL\\', \\'MR\\', \\'MC\\', \\'ML\\', \\'AMR\\', \\'AMC\\', \\'AML\\', \\'STC\\']],\\n                          value=role_dropdowns_defaults[index - 1] if role_dropdowns_defaults else None\\n            ),\\n            html.Label(f\\'Player {index}\\'),\\n            dcc.Dropdown(\\n                id=f\\'{prefix}-player{index}-dropdown\\',\\n                options=[{\\'label\\': player, \\'value\\': uid} for uid, player in team_dict.items()],\\n                value=player_dropdowns_defaults[index - 1] if player_dropdowns_defaults else None\\n            )\\n\\n        ], className=\\'sixcolumns\\'))\\n    return columns\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'--config\\', required=True, help=\"required argument --config <config>\")\\n    args = parser.parse_args()\\n\\n    with open(args.config, \\'r\\') as confhandle:\\n        config = yaml.safe_load(confhandle)\\n\\n    app_config.layout = create_config_layout()\\n    reload()\\n    #app_formations.layout = create_formation_layout()\\n\\n    server.run(debug=True)', metadata={'source': './DA_fm_analyze/view_formation.py'}),\n",
       " Document(page_content='import dash\\nfrom dash import html\\nfrom dash import dcc\\nimport argparse\\nimport yaml\\nimport pandas as pd\\n\\nfrom fmanalyze.aggregate.collect import create_full_squad_dfs\\nimport os\\n\\nfrom dash.dependencies import Input, Output, State\\n\\nfrom fmanalyze.roles.formation import read_formation_for_select\\nfrom fmanalyze.ui.dash_helper import fill_style_conditions, create_fm_data_table, create_formation_layout\\nfrom dash.dependencies import Input, Output\\n\\n\\nimport dash_html_components as html\\nfrom flask import Flask, render_template\\n\\n\\nconfig = None\\nserver = Flask(__name__)\\n\\n\\napp_squads = dash.Dash(__name__, server=server, url_base_pathname=\\'/squads/\\')\\napp_config = dash.Dash(__name__, server=server, url_base_pathname=\\'/config/\\')\\n\\n\\n\\nown_all_dfs = {}\\ncolor_dfs = {}\\ntab_dfs = {\"OCTAGON\" : [\\'octs\\', \\'gk_octs\\'],\\n           \"ATTRIBUTES\" : [\\'tec\\', \\'men\\', \\'phys\\', \\'goalk\\'],\\n           \"ABILITIES\" : [\\'tecabi\\', \\'menabi\\', \\'physabi\\']}\\n\\n\\n\\n@server.route(\\'/\\')\\ndef index():\\n    return render_template(\\'squad_index.html\\')\\n\\n\\n\\n@app_config.callback(\\n    Output(\\'redirect\\', \\'pathname\\'),\\n    [Input(\\'submit-button\\', \\'n_clicks\\')],\\n    [State(\\'role-dropdown\\', \\'value\\')]\\n)\\ndef on_button_click(n_clicks, value):\\n    if int(n_clicks) > 0:  # Cast n_clicks to an integer\\n        reload(value)\\n        return \\'/squads\\'\\n    return dash.no_update\\n\\n\\n@app_squads.callback(Output(\\'tab-content\\', \\'children\\'), [Input(\\'tabs\\', \\'value\\')])\\ndef render_content(tab):\\n    table_names = tab_dfs[tab]\\n    color_tables = [color_dfs[f\\'{table_name}_color\\'].drop(columns=[\\'UID\\']) for table_name in table_names]\\n    df_tables = [own_all_dfs[table_name].drop(columns=[\\'UID\\']) for table_name in table_names]\\n    style_condition_tables = [fill_style_conditions(color_table) for color_table in color_tables]\\n    # Set a fixed width for each column in pixels\\n\\n\\n    # Create the DataTable for df\\n    table_dfs = [create_fm_data_table(df_table, style_condition_table) for df_table, style_condition_table in zip(df_tables, style_condition_tables)]\\n\\n    return html.Div([\\n        html.H4(\\'Own Data\\'),\\n        html.Div([\\n            *table_dfs\\n        ], className=\\'tables-container\\')\\n    ])\\n\\n\\n\\n\\n\\ndef reload(value = None):\\n    basedir, teamname, rivalname = config[\"basedir\"], config[\"team\"], config.get(\"rival\", None)\\n    teamdir = os.path.join(basedir, \\'teams\\', teamname)\\n    quantilesdir = os.path.join(basedir, \\'quantiles\\')\\n    formation = config.get(\"formation\", None)\\n    create_full_squad_dfs(teamdir, quantilesdir, own_all_dfs, color_dfs, formation=None, selected_role=value)\\n    # Define the layout of the app\\n    app_squads.layout = create_formation_layout(tab_dfs, \\'OCTAGON\\')\\n\\n\\n\\n\\ndef create_config_layout():\\n    basedir, teamname, rivalname = config[\"basedir\"], config[\"team\"], config.get(\"rival\", None)\\n    teamdir = os.path.join(basedir, \\'teams\\', teamname)\\n\\n    team_dict = read_formation_for_select(teamdir, \\'full_squad.csv\\')\\n    columns = create_role_columns()\\n    return html.Div([\\n        html.H1(\\'Config\\'),\\n        html.Div(columns, className=\\'row\\'),\\n        html.Button(\\'Submit\\', id=\\'submit-button\\', n_clicks=0),\\n        dcc.Location(id=\\'redirect\\', refresh=True)\\n    ])\\n\\n\\ndef create_role_columns():\\n    columns = []\\n    columns.append(html.Div([\\n            html.Label(f\\'Role \\'),\\n            dcc.Dropdown(\\n                id=f\\'role-dropdown\\',\\n                options=[{\\'label\\': value, \\'value\\': value} for value in\\n                         [\\'GK\\', \\'DR\\', \\'DC\\', \\'DL\\', \\'WBR\\', \\'DM\\', \\'WBL\\', \\'MR\\', \\'MC\\', \\'ML\\', \\'AMR\\', \\'AMC\\', \\'AML\\', \\'STC\\']]\\n            ),\\n        ], className=\\'sixcolumns\\'))\\n    return columns\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'--config\\', required=True, help=\"required argument --config <config>\")\\n    args = parser.parse_args()\\n\\n    with open(args.config, \\'r\\') as confhandle:\\n        config = yaml.safe_load(confhandle)\\n\\n    app_config.layout = create_config_layout()\\n    reload()\\n\\n    server.run(debug=True)', metadata={'source': './DA_fm_analyze/view_squad.py'}),\n",
       " Document(page_content='.idea/*', metadata={'source': './DA_fm_analyze/.gitignore'}),\n",
       " Document(page_content='import argparse\\nimport os\\n\\nfrom fmanalyze.aggregate.main import create_dfs_for_basedir\\nfrom fmanalyze.attrs.instructions import *\\n\\npd.options.mode.chained_assignment = None\\nimport yaml\\nimport glob\\n\\n\\ndef create_all_dfs():\\n    getcwd = os.getcwd()\\n    data_dir = os.path.join(getcwd, \\'../../data\\')\\n    rtf_files = glob.glob(os.path.join(data_dir, \\'**/*.rtf\\'), recursive=True)\\n    for rtf_file in rtf_files:\\n        print(f\\'Processing {rtf_file}...\\')\\n        basedir = os.path.dirname(rtf_file)\\n        create_dfs_for_basedir(basedir)\\n\\n\\nif __name__ == \\'__main__\\':\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\', help=\\'Path to config file\\', default=None, required=False)\\n    args = parser.parse_args()\\n    if args.config == None:\\n        create_all_dfs()\\n    else:\\n        with open(args.config, \\'r\\') as confhandle:\\n            config = yaml.safe_load(confhandle)\\n\\n        basedir = config[\"basedir\"]\\n\\n        create_dfs_for_basedir(basedir)', metadata={'source': './DA_fm_analyze/scripts/create/create_dfs.py'}),\n",
       " Document(page_content='import cv2\\nimport pytesseract\\nfrom PIL import Image\\nfrom os.path import basename\\nimport numpy as np\\ntarget_text = \"GK,DR,DCR,DCL,DL,MCR,MCL,AMR,AC,AML,STC,S1,S2,S3,S4,S5,S6,S7\"\\ndistance_map= {}\\nimport Levenshtein\\ndef convert_image_dpi(image_path, dpi):\\n    img = Image.open(image_path)\\n    img_300dpi = img.convert(\"L\").resize(img.size, Image.ANTIALIAS)\\n    img_300dpi.save(\"temp_image_300dpi.png\", dpi=(dpi, dpi))\\n    return \"temp_image_300dpi.png\"\\n\\n\\ndef process_image(image_path):\\n    image_300dpi_path = convert_image_dpi(image_path, 300)\\n    image = cv2.imread(image_300dpi_path)\\n    cv2.imwrite(basename(image_path) + \".png\", image)\\n\\n    return image\\n\\ndef process_image_gray(image_path, block_size=15, c=5, blur=3):\\n    image = process_image(image_path)\\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n    image = cv2.bitwise_not(image)\\n\\n    #_, image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\\n    # Apply adaptive thresholding to highlight the text\\n    image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size , c)\\n    image = cv2.medianBlur(image, blur)\\n\\n    #kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\\n    #image = cv2.erode(image, kernel, iterations=1)\\n   # image = cv2.dilate(image, kernel, iterations=1)\\n\\n    cv2.imwrite(f\"{basename(image_path)}_{block_size}_{c}.png\", image)\\n    return image\\n\\n\\n# Load the image\\ndef read_image(image_path, process_method, custom_config):\\n    if process_method == process_image_gray:\\n        #for block_size in [3,5,7,9,11,13,15]:\\n        #    for c in [2,5,8,10,13,16]:\\n        #         for blur in [3,5,7,9,11,13,15]:\\n        for block_size in [9]:\\n            for c in [5]:\\n                blur = 5\\n                print(f\"block_size: {block_size}, c: {c}, blur: {blur}\")\\n                image = process_method(image_path, block_size=block_size, c=c)\\n                text = pytesseract.image_to_string(image, config=custom_config)\\n                text_line =\",\".join(text.splitlines())\\n                distance = Levenshtein.distance(text_line, target_text)\\n                print(text_line)\\n                print(distance)\\n    else:\\n        image = process_method(image_path)\\n        text = pytesseract.image_to_string(image, config=custom_config)\\n    return text\\n\\n\\nd2_text = read_image(image_path=\"images/d2.JPG\", process_method=process_image,\\n                      custom_config=r\"--psm 6 -c tessedit_char_whitelist=0123456789 -c load_system_dawg=0 -c load_freq_dawg=0\")\\n\\nd1_text = read_image(image_path=\"images/d1.JPG\", process_method=process_image_gray,\\n                      custom_config=r\"--psm 6  -c tessedit_char_whitelist=GKDCRLMSTA1234567 -c load_system_dawg=0 -c load_freq_dawg=0\")\\n\\n\\nprint(d2_text)\\nprint(d1_text)', metadata={'source': './DA_fm_analyze/scripts/tesseract/test_tesseract.py'}),\n",
       " Document(page_content='basedir: \"/Users/diego/projects/DA_fm_data/laliga/\"\\nteam: \"rsana\"\\nrival: \"barc\"\\nload_formation: \"formation_barca.csv\"\\nload_rival_formation: \"formation_barca.csv\"\\nsave_formation: \"formation_barca_2.csv\"\\nsave_rival_formation:  \"formation_rsana_2.csv\"', metadata={'source': './DA_fm_analyze/formations/rsana.yaml'}),\n",
       " Document(page_content='basedir: \"/Users/diego/projects/DA_fm_data/laliga/\"\\nteam: \"rsana\"\\n#formation: \"full_squad.csv\"\\nformation: \"squad_barca.csv\"', metadata={'source': './DA_fm_analyze/squads/rsana.yaml'}),\n",
       " Document(page_content='/* Define styles for the Tabs component */\\n.custom-tabs {\\n  display: flex;\\n  flex-wrap: wrap;\\n  border-bottom: 1px solid #d6d6d6;\\n  padding: 6px;\\n  font-family: Arial, sans-serif;\\n}\\n\\n.custom-tabs .custom-tab {\\n  margin-right: 10px;\\n  padding: 6px 12px;\\n  cursor: pointer;\\n}\\n\\n.custom-tabs .custom-tab--selected {\\n  color: white;\\n  background-color: #0074D9;\\n}\\n\\n/* Set display flex for the tables container */\\n.tables-container {\\n    display: flex;\\n    flex-wrap: wrap;\\n}\\n\\n/* Set display flex for the tables container */\\n.tables-container {\\n    display: flex;\\n    flex-wrap: wrap;\\n    align-items: flex-start;\\n}\\n\\n/* Set a margin and flex-basis for the DataTable */\\n.tables-container .dash-table-container {\\n    margin-right: 10px;  /* Adjust the value as needed */\\n    min-width: 250px;    /* Adjust the value as needed */\\n    max-width: 45%;      /* Adjust the value as needed */\\n    flex-grow: 1;\\n}', metadata={'source': './DA_fm_analyze/assets/app_squads.css'}),\n",
       " Document(page_content='.row {\\n    display: flex;\\n    margin-bottom: 15px;\\n}\\n\\n.sixcolumns {\\n    flex-basis: 50%;\\n    margin-left: 20px;\\n    margin-right: 20px;\\n    padding-left: 10px;\\n    padding-right: 10px;\\n}', metadata={'source': './DA_fm_analyze/assets/app_config.css'}),\n",
       " Document(page_content='/* Define styles for the Tabs component */\\n.custom-tabs {\\n  display: flex;\\n  flex-wrap: wrap;\\n  border-bottom: 1px solid #d6d6d6;\\n  padding: 6px;\\n  font-family: Arial, sans-serif;\\n}\\n\\n.custom-tabs .custom-tab {\\n  margin-right: 10px;\\n  padding: 6px 12px;\\n  cursor: pointer;\\n}\\n\\n.custom-tabs .custom-tab--selected {\\n  color: white;\\n  background-color: #0074D9;\\n}\\n\\n\\n/* Set display flex for the tables container */\\n.tables-container {\\n    display: flex;\\n    flex-wrap: wrap;\\n    align-items: flex-start;\\n}\\n\\n/* Set a margin and flex-basis for the DataTable */\\n.tables-container .dash-table-container {\\n    margin-right: 10px;  /* Adjust the value as needed */\\n    min-width: 250px;    /* Adjust the value as needed */\\n    max-width: 45%;      /* Adjust the value as needed */\\n    flex-grow: 1;\\n}', metadata={'source': './DA_fm_analyze/assets/app_formations.css'}),\n",
       " Document(page_content='ref: refs/heads/main', metadata={'source': './DA_fm_analyze/.git/HEAD'}),\n",
       " Document(page_content='[core]\\n\\trepositoryformatversion = 0\\n\\tfilemode = true\\n\\tbare = false\\n\\tlogallrefupdates = true\\n[remote \"origin\"]\\n\\turl = git@github.com:diegoami/DA_fm_analyze.git\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\n\\tremote = origin\\n\\tmerge = refs/heads/main', metadata={'source': './DA_fm_analyze/.git/config'}),\n",
       " Document(page_content='# pack-refs with: peeled fully-peeled sorted \\n1237fab74b9c56f8902a06e37e1ccd92a25b5f11 refs/remotes/origin/convert_qt\\n931d248c6a89d3b273136d6acb23d182e1d8621f refs/remotes/origin/formation_ltabs\\n931d248c6a89d3b273136d6acb23d182e1d8621f refs/remotes/origin/main', metadata={'source': './DA_fm_analyze/.git/packed-refs'}),\n",
       " Document(page_content=\"Unnamed repository; edit this file 'description' to name the repository.\", metadata={'source': './DA_fm_analyze/.git/description'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed.\\n# Called by \"git commit\" with no arguments.  The hook should\\n# exit with non-zero status after issuing an appropriate message if\\n# it wants to stop the commit.\\n#\\n# To enable this hook, rename this file to \"pre-commit\".\\n\\nif git rev-parse --verify HEAD >/dev/null 2>&1\\nthen\\n\\tagainst=HEAD\\nelse\\n\\t# Initial commit: diff against an empty tree object\\n\\tagainst=$(git hash-object -t tree /dev/null)\\nfi\\n\\n# If you want to allow non-ASCII filenames set this variable to true.\\nallownonascii=$(git config --type=bool hooks.allownonascii)\\n\\n# Redirect output to stderr.\\nexec 1>&2\\n\\n# Cross platform projects tend to avoid non-ASCII filenames; prevent\\n# them from being added to the repository. We exploit the fact that the\\n# printable range starts at the space character and ends with tilde.\\nif [ \"$allownonascii\" != \"true\" ] &&\\n\\t# Note that the use of brackets around a tr range is ok here, (it\\'s\\n\\t# even required, for portability to Solaris 10\\'s /usr/bin/tr), since\\n\\t# the square bracket bytes happen to fall in the designated range.\\n\\ttest $(git diff --cached --name-only --diff-filter=A -z $against |\\n\\t  LC_ALL=C tr -d \\'[ -~]\\\\0\\' | wc -c) != 0\\nthen\\n\\tcat <<\\\\EOF\\nError: Attempt to add a non-ASCII file name.\\n\\nThis can cause problems if you want to work with people on other platforms.\\n\\nTo be portable it is advisable to rename the file.\\n\\nIf you know what you are doing you can disable this check using:\\n\\n  git config hooks.allownonascii true\\nEOF\\n\\texit 1\\nfi\\n\\n# If there are whitespace errors, print the offending file names and fail.\\nexec git diff-index --check --cached $against --', metadata={'source': './DA_fm_analyze/.git/hooks/pre-commit.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to prepare the commit log message.\\n# Called by \"git commit\" with the name of the file that has the\\n# commit message, followed by the description of the commit\\n# message\\'s source.  The hook\\'s purpose is to edit the commit\\n# message file.  If the hook fails with a non-zero status,\\n# the commit is aborted.\\n#\\n# To enable this hook, rename this file to \"prepare-commit-msg\".\\n\\n# This hook includes three examples. The first one removes the\\n# \"# Please enter the commit message...\" help message.\\n#\\n# The second includes the output of \"git diff --name-status -r\"\\n# into the message, just before the \"git status\" output.  It is\\n# commented because it doesn\\'t cope with --amend or with squashed\\n# commits.\\n#\\n# The third example adds a Signed-off-by line to the message, that can\\n# still be edited.  This is rarely a good idea.\\n\\nCOMMIT_MSG_FILE=$1\\nCOMMIT_SOURCE=$2\\nSHA1=$3\\n\\n/usr/bin/perl -i.bak -ne \\'print unless(m/^. Please enter the commit message/..m/^#$/)\\' \"$COMMIT_MSG_FILE\"\\n\\n# case \"$COMMIT_SOURCE,$SHA1\" in\\n#  ,|template,)\\n#    /usr/bin/perl -i.bak -pe \\'\\n#       print \"\\\\n\" . `git diff --cached --name-status -r`\\n# \\t if /^#/ && $first++ == 0\\' \"$COMMIT_MSG_FILE\" ;;\\n#  *) ;;\\n# esac\\n\\n# SOB=$(git var GIT_COMMITTER_IDENT | sed -n \\'s/^\\\\(.*>\\\\).*$/Signed-off-by: \\\\1/p\\')\\n# git interpret-trailers --in-place --trailer \"$SOB\" \"$COMMIT_MSG_FILE\"\\n# if test -z \"$COMMIT_SOURCE\"\\n# then\\n#   /usr/bin/perl -i.bak -pe \\'print \"\\\\n\" if !$first_line++\\' \"$COMMIT_MSG_FILE\"\\n# fi', metadata={'source': './DA_fm_analyze/.git/hooks/prepare-commit-msg.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to prepare a packed repository for use over\\n# dumb transports.\\n#\\n# To enable this hook, rename this file to \"post-update\".\\n\\nexec git update-server-info', metadata={'source': './DA_fm_analyze/.git/hooks/post-update.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to check the commit log message taken by\\n# applypatch from an e-mail message.\\n#\\n# The hook should exit with non-zero status after issuing an\\n# appropriate message if it wants to stop the commit.  The hook is\\n# allowed to edit the commit message file.\\n#\\n# To enable this hook, rename this file to \"applypatch-msg\".\\n\\n. git-sh-setup\\ncommitmsg=\"$(git rev-parse --git-path hooks/commit-msg)\"\\ntest -x \"$commitmsg\" && exec \"$commitmsg\" ${1+\"$@\"}\\n:', metadata={'source': './DA_fm_analyze/.git/hooks/applypatch-msg.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed.\\n# Called by \"git merge\" with no arguments.  The hook should\\n# exit with non-zero status after issuing an appropriate message to\\n# stderr if it wants to stop the merge commit.\\n#\\n# To enable this hook, rename this file to \"pre-merge-commit\".\\n\\n. git-sh-setup\\ntest -x \"$GIT_DIR/hooks/pre-commit\" &&\\n        exec \"$GIT_DIR/hooks/pre-commit\"\\n:', metadata={'source': './DA_fm_analyze/.git/hooks/pre-merge-commit.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to check the commit log message.\\n# Called by \"git commit\" with one argument, the name of the file\\n# that has the commit message.  The hook should exit with non-zero\\n# status after issuing an appropriate message if it wants to stop the\\n# commit.  The hook is allowed to edit the commit message file.\\n#\\n# To enable this hook, rename this file to \"commit-msg\".\\n\\n# Uncomment the below to add a Signed-off-by line to the message.\\n# Doing this in a hook is a bad idea in general, but the prepare-commit-msg\\n# hook is more suited to it.\\n#\\n# SOB=$(git var GIT_AUTHOR_IDENT | sed -n \\'s/^\\\\(.*>\\\\).*$/Signed-off-by: \\\\1/p\\')\\n# grep -qs \"^$SOB\" \"$1\" || echo \"$SOB\" >> \"$1\"\\n\\n# This example catches duplicate Signed-off-by lines.\\n\\ntest \"\" = \"$(grep \\'^Signed-off-by: \\' \"$1\" |\\n\\t sort | uniq -c | sed -e \\'/^[ \\t]*1[ \\t]/d\\')\" || {\\n\\techo >&2 Duplicate Signed-off-by lines.\\n\\texit 1\\n}', metadata={'source': './DA_fm_analyze/.git/hooks/commit-msg.sample'}),\n",
       " Document(page_content='#!/usr/bin/perl\\n\\nuse strict;\\nuse warnings;\\nuse IPC::Open2;\\n\\n# An example hook script to integrate Watchman\\n# (https://facebook.github.io/watchman/) with git to speed up detecting\\n# new and modified files.\\n#\\n# The hook is passed a version (currently 2) and last update token\\n# formatted as a string and outputs to stdout a new update token and\\n# all files that have been modified since the update token. Paths must\\n# be relative to the root of the working tree and separated by a single NUL.\\n#\\n# To enable this hook, rename this file to \"query-watchman\" and set\\n# \\'git config core.fsmonitor .git/hooks/query-watchman\\'\\n#\\nmy ($version, $last_update_token) = @ARGV;\\n\\n# Uncomment for debugging\\n# print STDERR \"$0 $version $last_update_token\\\\n\";\\n\\n# Check the hook interface version\\nif ($version ne 2) {\\n\\tdie \"Unsupported query-fsmonitor hook version \\'$version\\'.\\\\n\" .\\n\\t    \"Falling back to scanning...\\\\n\";\\n}\\n\\nmy $git_work_tree = get_working_dir();\\n\\nmy $retry = 1;\\n\\nmy $json_pkg;\\neval {\\n\\trequire JSON::XS;\\n\\t$json_pkg = \"JSON::XS\";\\n\\t1;\\n} or do {\\n\\trequire JSON::PP;\\n\\t$json_pkg = \"JSON::PP\";\\n};\\n\\nlaunch_watchman();\\n\\nsub launch_watchman {\\n\\tmy $o = watchman_query();\\n\\tif (is_work_tree_watched($o)) {\\n\\t\\toutput_result($o->{clock}, @{$o->{files}});\\n\\t}\\n}\\n\\nsub output_result {\\n\\tmy ($clockid, @files) = @_;\\n\\n\\t# Uncomment for debugging watchman output\\n\\t# open (my $fh, \">\", \".git/watchman-output.out\");\\n\\t# binmode $fh, \":utf8\";\\n\\t# print $fh \"$clockid\\\\n@files\\\\n\";\\n\\t# close $fh;\\n\\n\\tbinmode STDOUT, \":utf8\";\\n\\tprint $clockid;\\n\\tprint \"\\\\0\";\\n\\tlocal $, = \"\\\\0\";\\n\\tprint @files;\\n}\\n\\nsub watchman_clock {\\n\\tmy $response = qx/watchman clock \"$git_work_tree\"/;\\n\\tdie \"Failed to get clock id on \\'$git_work_tree\\'.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $? != 0;\\n\\n\\treturn $json_pkg->new->utf8->decode($response);\\n}\\n\\nsub watchman_query {\\n\\tmy $pid = open2(\\\\*CHLD_OUT, \\\\*CHLD_IN, \\'watchman -j --no-pretty\\')\\n\\tor die \"open2() failed: $!\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\";\\n\\n\\t# In the query expression below we\\'re asking for names of files that\\n\\t# changed since $last_update_token but not from the .git folder.\\n\\t#\\n\\t# To accomplish this, we\\'re using the \"since\" generator to use the\\n\\t# recency index to select candidate nodes and \"fields\" to limit the\\n\\t# output to file names only. Then we\\'re using the \"expression\" term to\\n\\t# further constrain the results.\\n\\tif (substr($last_update_token, 0, 1) eq \"c\") {\\n\\t\\t$last_update_token = \"\\\\\"$last_update_token\\\\\"\";\\n\\t}\\n\\tmy $query = <<\"\\tEND\";\\n\\t\\t[\"query\", \"$git_work_tree\", {\\n\\t\\t\\t\"since\": $last_update_token,\\n\\t\\t\\t\"fields\": [\"name\"],\\n\\t\\t\\t\"expression\": [\"not\", [\"dirname\", \".git\"]]\\n\\t\\t}]\\n\\tEND\\n\\n\\t# Uncomment for debugging the watchman query\\n\\t# open (my $fh, \">\", \".git/watchman-query.json\");\\n\\t# print $fh $query;\\n\\t# close $fh;\\n\\n\\tprint CHLD_IN $query;\\n\\tclose CHLD_IN;\\n\\tmy $response = do {local $/; <CHLD_OUT>};\\n\\n\\t# Uncomment for debugging the watch response\\n\\t# open ($fh, \">\", \".git/watchman-response.json\");\\n\\t# print $fh $response;\\n\\t# close $fh;\\n\\n\\tdie \"Watchman: command returned no output.\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" if $response eq \"\";\\n\\tdie \"Watchman: command returned invalid output: $response\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" unless $response =~ /^\\\\{/;\\n\\n\\treturn $json_pkg->new->utf8->decode($response);\\n}\\n\\nsub is_work_tree_watched {\\n\\tmy ($output) = @_;\\n\\tmy $error = $output->{error};\\n\\tif ($retry > 0 and $error and $error =~ m/unable to resolve root .* directory (.*) is not watched/) {\\n\\t\\t$retry--;\\n\\t\\tmy $response = qx/watchman watch \"$git_work_tree\"/;\\n\\t\\tdie \"Failed to make watchman watch \\'$git_work_tree\\'.\\\\n\" .\\n\\t\\t    \"Falling back to scanning...\\\\n\" if $? != 0;\\n\\t\\t$output = $json_pkg->new->utf8->decode($response);\\n\\t\\t$error = $output->{error};\\n\\t\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\t\\t# Uncomment for debugging watchman output\\n\\t\\t# open (my $fh, \">\", \".git/watchman-output.out\");\\n\\t\\t# close $fh;', metadata={'source': './DA_fm_analyze/.git/hooks/fsmonitor-watchman.sample'}),\n",
       " Document(page_content='# Uncomment for debugging watchman output\\n\\t\\t# open (my $fh, \">\", \".git/watchman-output.out\");\\n\\t\\t# close $fh;\\n\\n\\t\\t# Watchman will always return all files on the first query so\\n\\t\\t# return the fast \"everything is dirty\" flag to git and do the\\n\\t\\t# Watchman query just to get it over with now so we won\\'t pay\\n\\t\\t# the cost in git to look up each individual file.\\n\\t\\tmy $o = watchman_clock();\\n\\t\\t$error = $output->{error};\\n\\n\\t\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\t\\toutput_result($o->{clock}, (\"/\"));\\n\\t\\t$last_update_token = $o->{clock};\\n\\n\\t\\teval { launch_watchman() };\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tdie \"Watchman: $error.\\\\n\" .\\n\\t\"Falling back to scanning...\\\\n\" if $error;\\n\\n\\treturn 1;\\n}\\n\\nsub get_working_dir {\\n\\tmy $working_dir;\\n\\tif ($^O =~ \\'msys\\' || $^O =~ \\'cygwin\\') {\\n\\t\\t$working_dir = Win32::GetCwd();\\n\\t\\t$working_dir =~ tr/\\\\\\\\/\\\\//;\\n\\t} else {\\n\\t\\trequire Cwd;\\n\\t\\t$working_dir = Cwd::cwd();\\n\\t}\\n\\n\\treturn $working_dir;\\n}', metadata={'source': './DA_fm_analyze/.git/hooks/fsmonitor-watchman.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n\\n# An example hook script to update a checked-out tree on a git push.\\n#\\n# This hook is invoked by git-receive-pack(1) when it reacts to git\\n# push and updates reference(s) in its repository, and when the push\\n# tries to update the branch that is currently checked out and the\\n# receive.denyCurrentBranch configuration variable is set to\\n# updateInstead.\\n#\\n# By default, such a push is refused if the working tree and the index\\n# of the remote repository has any difference from the currently\\n# checked out commit; when both the working tree and the index match\\n# the current commit, they are updated to match the newly pushed tip\\n# of the branch. This hook is to be used to override the default\\n# behaviour; however the code below reimplements the default behaviour\\n# as a starting point for convenient modification.\\n#\\n# The hook receives the commit with which the tip of the current\\n# branch is going to be updated:\\ncommit=$1\\n\\n# It can exit with a non-zero status to refuse the push (when it does\\n# so, it must not modify the index or the working tree).\\ndie () {\\n\\techo >&2 \"$*\"\\n\\texit 1\\n}\\n\\n# Or it can make any necessary changes to the working tree and to the\\n# index to bring them to the desired state when the tip of the current\\n# branch is updated to the new commit, and exit with a zero status.\\n#\\n# For example, the hook can simply run git read-tree -u -m HEAD \"$1\"\\n# in order to emulate git fetch that is run in the reverse direction\\n# with git push, as the two-tree form of git read-tree -u -m is\\n# essentially the same as git switch or git checkout that switches\\n# branches while keeping the local changes in the working tree that do\\n# not interfere with the difference between the branches.\\n\\n# The below is a more-or-less exact translation to shell of the C code\\n# for the default behaviour for git\\'s push-to-checkout hook defined in\\n# the push_to_deploy() function in builtin/receive-pack.c.\\n#\\n# Note that the hook will be executed from the repository directory,\\n# not from the working tree, so if you want to perform operations on\\n# the working tree, you will have to adapt your code accordingly, e.g.\\n# by adding \"cd ..\" or using relative paths.\\n\\nif ! git update-index -q --ignore-submodules --refresh\\nthen\\n\\tdie \"Up-to-date check failed\"\\nfi\\n\\nif ! git diff-files --quiet --ignore-submodules --\\nthen\\n\\tdie \"Working directory has unstaged changes\"\\nfi\\n\\n# This is a rough translation of:\\n#\\n#   head_has_history() ? \"HEAD\" : EMPTY_TREE_SHA1_HEX\\nif git cat-file -e HEAD 2>/dev/null\\nthen\\n\\thead=HEAD\\nelse\\n\\thead=$(git hash-object -t tree --stdin </dev/null)\\nfi\\n\\nif ! git diff-index --quiet --cached --ignore-submodules $head --\\nthen\\n\\tdie \"Working directory has staged changes\"\\nfi\\n\\nif ! git read-tree -u -m \"$commit\"\\nthen\\n\\tdie \"Could not update working tree to new HEAD\"\\nfi', metadata={'source': './DA_fm_analyze/.git/hooks/push-to-checkout.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# Copyright (c) 2006, 2008 Junio C Hamano\\n#\\n# The \"pre-rebase\" hook is run just before \"git rebase\" starts doing\\n# its job, and can prevent the command from running by exiting with\\n# non-zero status.\\n#\\n# The hook is called with the following parameters:\\n#\\n# $1 -- the upstream the series was forked from.\\n# $2 -- the branch being rebased (or empty when rebasing the current branch).\\n#\\n# This sample shows how to prevent topic branches that are already\\n# merged to \\'next\\' branch from getting rebased, because allowing it\\n# would result in rebasing already published history.\\n\\npublish=next\\nbasebranch=\"$1\"\\nif test \"$#\" = 2\\nthen\\n\\ttopic=\"refs/heads/$2\"\\nelse\\n\\ttopic=`git symbolic-ref HEAD` ||\\n\\texit 0 ;# we do not interrupt rebasing detached HEAD\\nfi\\n\\ncase \"$topic\" in\\nrefs/heads/??/*)\\n\\t;;\\n*)\\n\\texit 0 ;# we do not interrupt others.\\n\\t;;\\nesac\\n\\n# Now we are dealing with a topic branch being rebased\\n# on top of master.  Is it OK to rebase it?\\n\\n# Does the topic really exist?\\ngit show-ref -q \"$topic\" || {\\n\\techo >&2 \"No such branch $topic\"\\n\\texit 1\\n}\\n\\n# Is topic fully merged to master?\\nnot_in_master=`git rev-list --pretty=oneline ^master \"$topic\"`\\nif test -z \"$not_in_master\"\\nthen\\n\\techo >&2 \"$topic is fully merged to master; better remove it.\"\\n\\texit 1 ;# we could allow it, but there is no point.\\nfi\\n\\n# Is topic ever merged to next?  If so you should not be rebasing it.\\nonly_next_1=`git rev-list ^master \"^$topic\" ${publish} | sort`\\nonly_next_2=`git rev-list ^master           ${publish} | sort`\\nif test \"$only_next_1\" = \"$only_next_2\"\\nthen\\n\\tnot_in_topic=`git rev-list \"^$topic\" master`\\n\\tif test -z \"$not_in_topic\"\\n\\tthen\\n\\t\\techo >&2 \"$topic is already up to date with master\"\\n\\t\\texit 1 ;# we could allow it, but there is no point.\\n\\telse\\n\\t\\texit 0\\n\\tfi\\nelse\\n\\tnot_in_next=`git rev-list --pretty=oneline ^${publish} \"$topic\"`\\n\\t/usr/bin/perl -e \\'\\n\\t\\tmy $topic = $ARGV[0];\\n\\t\\tmy $msg = \"* $topic has commits already merged to public branch:\\\\n\";\\n\\t\\tmy (%not_in_next) = map {\\n\\t\\t\\t/^([0-9a-f]+) /;\\n\\t\\t\\t($1 => 1);\\n\\t\\t} split(/\\\\n/, $ARGV[1]);\\n\\t\\tfor my $elem (map {\\n\\t\\t\\t\\t/^([0-9a-f]+) (.*)$/;\\n\\t\\t\\t\\t[$1 => $2];\\n\\t\\t\\t} split(/\\\\n/, $ARGV[2])) {\\n\\t\\t\\tif (!exists $not_in_next{$elem->[0]}) {\\n\\t\\t\\t\\tif ($msg) {\\n\\t\\t\\t\\t\\tprint STDERR $msg;\\n\\t\\t\\t\\t\\tundef $msg;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tprint STDERR \" $elem->[1]\\\\n\";\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\' \"$topic\" \"$not_in_next\" \"$not_in_master\"\\n\\texit 1\\nfi\\n\\n<<\\\\DOC_END\\n\\nThis sample hook safeguards topic branches that have been\\npublished from being rewound.\\n\\nThe workflow assumed here is:\\n\\n * Once a topic branch forks from \"master\", \"master\" is never\\n   merged into it again (either directly or indirectly).\\n\\n * Once a topic branch is fully cooked and merged into \"master\",\\n   it is deleted.  If you need to build on top of it to correct\\n   earlier mistakes, a new topic branch is created by forking at\\n   the tip of the \"master\".  This is not strictly necessary, but\\n   it makes it easier to keep your history simple.\\n\\n * Whenever you need to test or publish your changes to topic\\n   branches, merge them into \"next\" branch.\\n\\nThe script, being an example, hardcodes the publish branch name\\nto be \"next\", but it is trivial to make it configurable via\\n$GIT_DIR/config mechanism.\\n\\nWith this workflow, you would want to know:\\n\\n(1) ... if a topic branch has ever been merged to \"next\".  Young\\n    topic branches can have stupid mistakes you would rather\\n    clean up before publishing, and things that have not been\\n    merged into other branches can be easily rebased without\\n    affecting other people.  But once it is published, you would\\n    not want to rewind it.\\n\\n(2) ... if a topic branch has been fully merged to \"master\".\\n    Then you can delete it.  More importantly, you should not\\n    build on top of it -- other people may already want to\\n    change things related to the topic as patches against your\\n    \"master\", so if you need further changes, it is better to\\n    fork the topic (perhaps with the same name) afresh from the\\n    tip of \"master\".\\n\\nLet\\'s look at this example:', metadata={'source': './DA_fm_analyze/.git/hooks/pre-rebase.sample'}),\n",
       " Document(page_content='Let\\'s look at this example:\\n\\n\\t\\t   o---o---o---o---o---o---o---o---o---o \"next\"\\n\\t\\t  /       /           /           /\\n\\t\\t /   a---a---b A     /           /\\n\\t\\t/   /               /           /\\n\\t       /   /   c---c---c---c B         /\\n\\t      /   /   /             \\\\         /\\n\\t     /   /   /   b---b C     \\\\       /\\n\\t    /   /   /   /             \\\\     /\\n    ---o---o---o---o---o---o---o---o---o---o---o \"master\"\\n\\n\\nA, B and C are topic branches.\\n\\n * A has one fix since it was merged up to \"next\".\\n\\n * B has finished.  It has been fully merged up to \"master\" and \"next\",\\n   and is ready to be deleted.\\n\\n * C has not merged to \"next\" at all.\\n\\nWe would want to allow C to be rebased, refuse A, and encourage\\nB to be deleted.\\n\\nTo compute (1):\\n\\n\\tgit rev-list ^master ^topic next\\n\\tgit rev-list ^master        next\\n\\n\\tif these match, topic has not merged in next at all.\\n\\nTo compute (2):\\n\\n\\tgit rev-list master..topic\\n\\n\\tif this is empty, it is fully merged to \"master\".\\n\\nDOC_END', metadata={'source': './DA_fm_analyze/.git/hooks/pre-rebase.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n\\n# An example hook script to verify what is about to be pushed.  Called by \"git\\n# push\" after it has checked the remote status, but before anything has been\\n# pushed.  If this script exits with a non-zero status nothing will be pushed.\\n#\\n# This hook is called with the following parameters:\\n#\\n# $1 -- Name of the remote to which the push is being done\\n# $2 -- URL to which the push is being done\\n#\\n# If pushing without using a named remote those arguments will be equal.\\n#\\n# Information about the commits which are being pushed is supplied as lines to\\n# the standard input in the form:\\n#\\n#   <local ref> <local oid> <remote ref> <remote oid>\\n#\\n# This sample shows how to prevent push of commits where the log message starts\\n# with \"WIP\" (work in progress).\\n\\nremote=\"$1\"\\nurl=\"$2\"\\n\\nzero=$(git hash-object --stdin </dev/null | tr \\'[0-9a-f]\\' \\'0\\')\\n\\nwhile read local_ref local_oid remote_ref remote_oid\\ndo\\n\\tif test \"$local_oid\" = \"$zero\"\\n\\tthen\\n\\t\\t# Handle delete\\n\\t\\t:\\n\\telse\\n\\t\\tif test \"$remote_oid\" = \"$zero\"\\n\\t\\tthen\\n\\t\\t\\t# New branch, examine all commits\\n\\t\\t\\trange=\"$local_oid\"\\n\\t\\telse\\n\\t\\t\\t# Update to existing branch, examine new commits\\n\\t\\t\\trange=\"$remote_oid..$local_oid\"\\n\\t\\tfi\\n\\n\\t\\t# Check for WIP commit\\n\\t\\tcommit=$(git rev-list -n 1 --grep \\'^WIP\\' \"$range\")\\n\\t\\tif test -n \"$commit\"\\n\\t\\tthen\\n\\t\\t\\techo >&2 \"Found WIP commit in $local_ref, not pushing\"\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\tfi\\ndone\\n\\nexit 0', metadata={'source': './DA_fm_analyze/.git/hooks/pre-push.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to verify what is about to be committed\\n# by applypatch from an e-mail message.\\n#\\n# The hook should exit with non-zero status after issuing an\\n# appropriate message if it wants to stop the commit.\\n#\\n# To enable this hook, rename this file to \"pre-applypatch\".\\n\\n. git-sh-setup\\nprecommit=\"$(git rev-parse --git-path hooks/pre-commit)\"\\ntest -x \"$precommit\" && exec \"$precommit\" ${1+\"$@\"}\\n:', metadata={'source': './DA_fm_analyze/.git/hooks/pre-applypatch.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to make use of push options.\\n# The example simply echoes all push options that start with \\'echoback=\\'\\n# and rejects all pushes when the \"reject\" push option is used.\\n#\\n# To enable this hook, rename this file to \"pre-receive\".\\n\\nif test -n \"$GIT_PUSH_OPTION_COUNT\"\\nthen\\n\\ti=0\\n\\twhile test \"$i\" -lt \"$GIT_PUSH_OPTION_COUNT\"\\n\\tdo\\n\\t\\teval \"value=\\\\$GIT_PUSH_OPTION_$i\"\\n\\t\\tcase \"$value\" in\\n\\t\\techoback=*)\\n\\t\\t\\techo \"echo from the pre-receive-hook: ${value#*=}\" >&2\\n\\t\\t\\t;;\\n\\t\\treject)\\n\\t\\t\\texit 1\\n\\t\\tesac\\n\\t\\ti=$((i + 1))\\n\\tdone\\nfi', metadata={'source': './DA_fm_analyze/.git/hooks/pre-receive.sample'}),\n",
       " Document(page_content='#!/bin/sh\\n#\\n# An example hook script to block unannotated tags from entering.\\n# Called by \"git receive-pack\" with arguments: refname sha1-old sha1-new\\n#\\n# To enable this hook, rename this file to \"update\".\\n#\\n# Config\\n# ------\\n# hooks.allowunannotated\\n#   This boolean sets whether unannotated tags will be allowed into the\\n#   repository.  By default they won\\'t be.\\n# hooks.allowdeletetag\\n#   This boolean sets whether deleting tags will be allowed in the\\n#   repository.  By default they won\\'t be.\\n# hooks.allowmodifytag\\n#   This boolean sets whether a tag may be modified after creation. By default\\n#   it won\\'t be.\\n# hooks.allowdeletebranch\\n#   This boolean sets whether deleting branches will be allowed in the\\n#   repository.  By default they won\\'t be.\\n# hooks.denycreatebranch\\n#   This boolean sets whether remotely creating branches will be denied\\n#   in the repository.  By default this is allowed.\\n#\\n\\n# --- Command line\\nrefname=\"$1\"\\noldrev=\"$2\"\\nnewrev=\"$3\"\\n\\n# --- Safety check\\nif [ -z \"$GIT_DIR\" ]; then\\n\\techo \"Don\\'t run this script from the command line.\" >&2\\n\\techo \" (if you want, you could supply GIT_DIR then run\" >&2\\n\\techo \"  $0 <ref> <oldrev> <newrev>)\" >&2\\n\\texit 1\\nfi\\n\\nif [ -z \"$refname\" -o -z \"$oldrev\" -o -z \"$newrev\" ]; then\\n\\techo \"usage: $0 <ref> <oldrev> <newrev>\" >&2\\n\\texit 1\\nfi\\n\\n# --- Config\\nallowunannotated=$(git config --type=bool hooks.allowunannotated)\\nallowdeletebranch=$(git config --type=bool hooks.allowdeletebranch)\\ndenycreatebranch=$(git config --type=bool hooks.denycreatebranch)\\nallowdeletetag=$(git config --type=bool hooks.allowdeletetag)\\nallowmodifytag=$(git config --type=bool hooks.allowmodifytag)\\n\\n# check for no description\\nprojectdesc=$(sed -e \\'1q\\' \"$GIT_DIR/description\")\\ncase \"$projectdesc\" in\\n\"Unnamed repository\"* | \"\")\\n\\techo \"*** Project description file hasn\\'t been set\" >&2\\n\\texit 1\\n\\t;;\\nesac\\n\\n# --- Check types\\n# if $newrev is 0000...0000, it\\'s a commit to delete a ref.\\nzero=$(git hash-object --stdin </dev/null | tr \\'[0-9a-f]\\' \\'0\\')\\nif [ \"$newrev\" = \"$zero\" ]; then\\n\\tnewrev_type=delete\\nelse\\n\\tnewrev_type=$(git cat-file -t $newrev)\\nfi\\n\\ncase \"$refname\",\"$newrev_type\" in\\n\\trefs/tags/*,commit)\\n\\t\\t# un-annotated tag\\n\\t\\tshort_refname=${refname##refs/tags/}\\n\\t\\tif [ \"$allowunannotated\" != \"true\" ]; then\\n\\t\\t\\techo \"*** The un-annotated tag, $short_refname, is not allowed in this repository\" >&2\\n\\t\\t\\techo \"*** Use \\'git tag [ -a | -s ]\\' for tags you want to propagate.\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/tags/*,delete)\\n\\t\\t# delete tag\\n\\t\\tif [ \"$allowdeletetag\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a tag is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/tags/*,tag)\\n\\t\\t# annotated tag\\n\\t\\tif [ \"$allowmodifytag\" != \"true\" ] && git rev-parse $refname > /dev/null 2>&1\\n\\t\\tthen\\n\\t\\t\\techo \"*** Tag \\'$refname\\' already exists.\" >&2\\n\\t\\t\\techo \"*** Modifying a tag is not allowed in this repository.\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/heads/*,commit)\\n\\t\\t# branch\\n\\t\\tif [ \"$oldrev\" = \"$zero\" -a \"$denycreatebranch\" = \"true\" ]; then\\n\\t\\t\\techo \"*** Creating a branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/heads/*,delete)\\n\\t\\t# delete branch\\n\\t\\tif [ \"$allowdeletebranch\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\trefs/remotes/*,commit)\\n\\t\\t# tracking branch\\n\\t\\t;;\\n\\trefs/remotes/*,delete)\\n\\t\\t# delete tracking branch\\n\\t\\tif [ \"$allowdeletebranch\" != \"true\" ]; then\\n\\t\\t\\techo \"*** Deleting a tracking branch is not allowed in this repository\" >&2\\n\\t\\t\\texit 1\\n\\t\\tfi\\n\\t\\t;;\\n\\t*)\\n\\t\\t# Anything else (is there anything else?)\\n\\t\\techo \"*** Update hook: unknown type of update to ref $refname of type $newrev_type\" >&2\\n\\t\\texit 1\\n\\t\\t;;\\nesac\\n\\n# --- Finished\\nexit 0', metadata={'source': './DA_fm_analyze/.git/hooks/update.sample'}),\n",
       " Document(page_content='931d248c6a89d3b273136d6acb23d182e1d8621f', metadata={'source': './DA_fm_analyze/.git/refs/heads/main'}),\n",
       " Document(page_content='ref: refs/remotes/origin/main', metadata={'source': './DA_fm_analyze/.git/refs/remotes/origin/HEAD'}),\n",
       " Document(page_content='0000000000000000000000000000000000000000 931d248c6a89d3b273136d6acb23d182e1d8621f Diego Amicabile <diego.amicabile@gmail.com> 1684098882 +0200\\tclone: from github.com:diegoami/DA_fm_analyze.git', metadata={'source': './DA_fm_analyze/.git/logs/HEAD'}),\n",
       " Document(page_content='0000000000000000000000000000000000000000 931d248c6a89d3b273136d6acb23d182e1d8621f Diego Amicabile <diego.amicabile@gmail.com> 1684098882 +0200\\tclone: from github.com:diegoami/DA_fm_analyze.git', metadata={'source': './DA_fm_analyze/.git/logs/refs/heads/main'}),\n",
       " Document(page_content='0000000000000000000000000000000000000000 931d248c6a89d3b273136d6acb23d182e1d8621f Diego Amicabile <diego.amicabile@gmail.com> 1684098882 +0200\\tclone: from github.com:diegoami/DA_fm_analyze.git', metadata={'source': './DA_fm_analyze/.git/logs/refs/remotes/origin/HEAD'}),\n",
       " Document(page_content=\"# git ls-files --others --exclude-from=.git/info/exclude\\n# Lines that start with '#' are comments.\\n# For a project mostly in C, the following would be a good set of\\n# exclude patterns (uncomment them if you want to use them):\\n# *.[oa]\\n# *~\", metadata={'source': './DA_fm_analyze/.git/info/exclude'}),\n",
       " Document(page_content=\"from PyQt5.QtCore import QFile, QFileInfo, Qt\\nfrom PyQt5.QtGui import QStandardItem, QStandardItemModel\\nfrom PyQt5.QtWidgets import QApplication, QHeaderView, QTableView\\n\\n\\nclass FreezeTableWidget(QTableView):\\n    def __init__(self, model):\\n        super(FreezeTableWidget, self).__init__()\\n        self.setModel(model)\\n        self.frozenTableView = QTableView(self)\\n        self.init()\\n        self.horizontalHeader().sectionResized.connect(self.updateSectionWidth)\\n        self.verticalHeader().sectionResized.connect(self.updateSectionHeight)\\n        self.frozenTableView.verticalScrollBar().valueChanged.connect(\\n            self.verticalScrollBar().setValue)\\n        self.verticalScrollBar().valueChanged.connect(\\n            self.frozenTableView.verticalScrollBar().setValue)\\n\\n    def init(self):\\n        self.frozenTableView.setModel(self.model())\\n        self.frozenTableView.setFocusPolicy(Qt.NoFocus)\\n        self.frozenTableView.verticalHeader().hide()\\n        self.frozenTableView.horizontalHeader().setSectionResizeMode(\\n                QHeaderView.Fixed)\\n        self.viewport().stackUnder(self.frozenTableView)\\n\\n        self.frozenTableView.setStyleSheet('''\\n            QTableView { border: none;\\n                         background-color: #8EDE21;\\n                         selection-background-color: #999;\\n            }''') # for demo purposes\\n\\n        self.frozenTableView.setSelectionModel(self.selectionModel())\\n        for col in range(1, self.model().columnCount()):\\n            self.frozenTableView.setColumnHidden(col, True)\\n        self.frozenTableView.setColumnWidth(0, self.columnWidth(0))\\n        self.frozenTableView.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\\n        self.frozenTableView.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\\n        self.frozenTableView.show()\\n        self.updateFrozenTableGeometry()\\n        self.setHorizontalScrollMode(self.ScrollPerPixel)\\n        self.setVerticalScrollMode(self.ScrollPerPixel)\\n        self.frozenTableView.setVerticalScrollMode(self.ScrollPerPixel)\\n\\n    def updateSectionWidth(self, logicalIndex, oldSize, newSize):\\n        if self.logicalIndex == 0:\\n            self.frozenTableView.setColumnWidth(0, newSize)\\n            self.updateFrozenTableGeometry()\\n\\n    def updateSectionHeight(self, logicalIndex, oldSize, newSize):\\n        self.frozenTableView.setRowHeight(logicalIndex, newSize)\\n\\n    def resizeEvent(self, event):\\n        super(FreezeTableWidget, self).resizeEvent(event)\\n        self.updateFrozenTableGeometry()\\n\\n    def moveCursor(self, cursorAction, modifiers):\\n        current = super(FreezeTableWidget, self).moveCursor(cursorAction, modifiers)\\n        if (cursorAction == self.MoveLeft and\\n                self.current.column() > 0 and\\n                self.visualRect(current).topLeft().x() <\\n                    self.frozenTableView.columnWidth(0)):\\n            newValue = (self.horizontalScrollBar().value() +\\n                        self.visualRect(current).topLeft().x() -\\n                        self.frozenTableView.columnWidth(0))\\n            self.horizontalScrollBar().setValue(newValue)\\n        return current\\n\\n    def scrollTo(self, index, hint):\\n        if index.column() > 0:\\n            super(FreezeTableWidget, self).scrollTo(index, hint)\\n\\n    def updateFrozenTableGeometry(self):\\n        self.frozenTableView.setGeometry(\\n                self.verticalHeader().width() + self.frameWidth(),\\n                self.frameWidth(), self.columnWidth(0),\\n                self.viewport().height() + self.horizontalHeader().height())\", metadata={'source': './DA_fm_analyze/fmanalyze/ui/frozentable.py'}),\n",
       " Document(page_content='import dash_table\\nfrom dash import html, dcc\\n\\n\\ncolor_map = {-2: \\'red\\', -1: \\'orange\\', 0: \\'yellow\\', 1: \\'lightgreen\\', 2: \\'darkgreen\\'}\\n\\ndef fill_style_conditions(color_df):\\n    style_conditions = []\\n    for index, row in color_df.iterrows():\\n        for icol, col in enumerate(color_df.columns):\\n            if col not in [\\'Player\\', \\'UID\\', \\'Position\\']:\\n                content = row[col]\\n                style_conditions.append({\\n                    \\'if\\': {\\'row_index\\': index, \\'column_id\\': col},\\n                    \\'backgroundColor\\': color_map[content],\\n                })\\n    return style_conditions\\n\\n\\ndef create_fm_data_table(df, style_conditions, column_width=30):\\n\\n    # Modify the style_cell in both DataTables\\n    style_cell = {\\n        \\'textAlign\\': \\'center\\',\\n        \\'minWidth\\': f\\'{column_width}px\\',\\n        \\'maxWidth\\': \\'150px\\',\\n        \\'width\\': f\\'{column_width}px\\',\\n        \\'padding\\': \\'8px\\',\\n        \\'border\\': \\'1px solid #ddd\\',\\n        \\'font-family\\': \\'Arial, sans-serif\\'\\n    }\\n\\n    # Modify the style_header in both DataTables\\n    style_header = {\\n        \\'textAlign\\': \\'center\\',\\n        \\'backgroundColor\\': \\'#f2f2f2\\',\\n        \\'fontWeight\\': \\'bold\\',\\n        \\'padding\\': \\'8px\\',\\n        \\'border\\': \\'1px solid #ddd\\',\\n        \\'font-family\\': \\'Arial, sans-serif\\'\\n    }\\n\\n    return dash_table.DataTable(\\n        columns=[{\"name\": i, \"id\": i} for i in df.columns],\\n        data=df.to_dict(\\'records\\'),\\n        style_data_conditional=style_conditions,\\n        style_cell=style_cell,\\n        style_header=style_header\\n    )\\n\\n\\ndef create_formation_layout(dfs, value):\\n    return html.Div([\\n        dcc.Tabs(id=\\'tabs\\', value=value, children=[\\n            dcc.Tab(label=name, value=name, className=\\'custom-tab\\',\\n                    selected_className=\\'custom-tab--selected\\') for name in dfs.keys()\\n        ]),\\n        html.Div(id=\\'tab-content\\')\\n    ])', metadata={'source': './DA_fm_analyze/fmanalyze/ui/dash_helper.py'}),\n",
       " Document(page_content='import os\\n\\nimport pandas as pd\\n\\nfrom fmanalyze.roles.extract import COL_ROLES\\n\\n\\ndef combine_dfs(teamdir, roledir, filename):\\n    dataframes = {}\\n    for teams in os.listdir(teamdir):\\n        attr_file = os.path.join(teamdir, teams, filename)\\n        if os.path.exists(attr_file):\\n            dataframes[teams] = pd.read_csv(attr_file)\\n    # Initialize an empty DataFrame to store the concatenated data\\n    combined_df = pd.DataFrame()\\n    # Iterate through the dictionary, adding a new column with the key and concatenating the DataFrames\\n    for team, df in dataframes.items():\\n        df_with_key = df.copy()  # Create a copy to avoid modifying the original DataFrames\\n        df_with_key[\\'Team\\'] = team  # Add a new column with the key\\n        combined_df = pd.concat([combined_df, df_with_key], ignore_index=True)\\n    # The combined_df now contains all the DataFrames with an additional \\'key\\' column\\n    combined_df.to_csv(os.path.join(roledir, filename), index=False)\\n\\n    return combined_df\\n\\n\\ndef generate_combin_for_roles(role, league_attrs, league_octs, league_gk_octs, league_abis, league_roles, roledir):\\n\\n    league_players_in_role = league_roles[league_roles[role] == 1][[\\'Player\\', \\'UID\\', \\'Team\\']]\\n\\n    league_attrs_in_role = league_attrs.merge(league_players_in_role, on=[\\'Player\\', \\'UID\\', \\'Team\\'])\\n    league_attrs_in_role.to_csv(os.path.join(roledir, f\\'{role}_attrs.csv\\'), index=False)\\n\\n    league_octs_in_role = league_octs.merge(league_players_in_role, on=[\\'Player\\', \\'UID\\', \\'Team\\'])\\n    league_octs_in_role.to_csv(os.path.join(roledir, f\\'{role}_octs.csv\\'), index=False)\\n\\n    league_gk_octs_in_role = league_gk_octs.merge(league_players_in_role, on=[\\'Player\\', \\'UID\\', \\'Team\\'])\\n    league_gk_octs_in_role.to_csv(os.path.join(roledir, f\\'{role}_gk_octs.csv\\'), index=False)\\n\\n    league_abis_in_role = league_abis.merge(league_players_in_role, on=[\\'Player\\', \\'UID\\', \\'Team\\'])\\n    league_abis_in_role.to_csv(os.path.join(roledir, f\\'{role}_abis.csv\\'), index=False)\\n\\n    league_roles_in_role = league_roles.merge(league_players_in_role, on=[\\'Player\\', \\'UID\\', \\'Team\\'])\\n    league_roles_in_role.to_csv(os.path.join(roledir, f\\'{role}_roles.csv\\'), index=False)\\n\\n\\ndef generate_all_combinations(roledir, teamdir):\\n    league_attrs = combine_dfs(teamdir, roledir, \\'all_attrs.csv\\')\\n    league_octs = combine_dfs(teamdir, roledir, \\'octs.csv\\')\\n    league_gk_octs = combine_dfs(teamdir, roledir, \\'gk_octs.csv\\')\\n\\n    league_abis = combine_dfs(teamdir, roledir, \\'abis.csv\\')\\n    #league_exps = combine_dfs(teamdir, roledir, \\'exp.csv\\')\\n    league_roles = combine_dfs(teamdir, roledir, \\'roles.csv\\')\\n    print(\"Combined all dfs\")\\n    for role in COL_ROLES:\\n        print(\"Generating combinations for role: \", role)\\n        generate_combin_for_roles(role, league_attrs, league_octs, league_gk_octs, league_abis, league_roles, roledir)', metadata={'source': './DA_fm_analyze/fmanalyze/stats/leagues.py'}),\n",
       " Document(page_content='import os\\n\\nimport pandas\\nimport pandas as pd\\n\\nfrom fmanalyze.roles.extract import COL_ROLES\\n\\n\\ndef save_stats_for_attrs(rolesdir, quantilesdir, filesufix):\\n    all_csvs = [f\\'{role}_{filesufix}\\' for role in COL_ROLES]\\n    all_dfs = {}\\n    os.makedirs(quantilesdir, exist_ok=True)\\n    for csv in all_csvs:\\n        csv_df = pandas.read_csv(f\\'{rolesdir}/{csv}.csv\\')\\n        all_dfs[csv] = csv_df\\n        quantiles_df = pd.DataFrame(columns=[\"DF\", \"COL\", \"Q0\", \"Q20\", \"Q40\", \"Q60\", \"Q80\", \"Q100\"])\\n        print(csv_df.dtypes)\\n        for col in csv_df.columns:\\n            if (csv_df[col].dtype == \"int64\" or csv_df[col].dtype == \"float64\") and col != \"UID\":\\n                new_row = {\\n                    \"DF\": csv,\\n                    \"COL\": col,\\n                    \"Q0\": csv_df[col].quantile(0, interpolation=\\'nearest\\'),\\n                    \"Q20\": csv_df[col].quantile(0.2, interpolation=\\'nearest\\'),\\n                    \"Q40\": csv_df[col].quantile(0.4, interpolation=\\'nearest\\'),\\n                    \"Q60\": csv_df[col].quantile(0.6, interpolation=\\'nearest\\'),\\n                    \"Q80\": csv_df[col].quantile(0.8, interpolation=\\'nearest\\'),\\n                    \"Q100\": csv_df[col].quantile(1, interpolation=\\'nearest\\')\\n                }\\n                quantiles_csv_df_cols = pd.DataFrame([new_row])\\n\\n                # add a new row to the dataframe\\n                quantiles_df = pd.concat([quantiles_df, quantiles_csv_df_cols], ignore_index=True)\\n        print(f\"Saving quantiles_{csv}.csv\")\\n        quantiles_df.to_csv(f\\'{quantilesdir}/quantiles_{csv}.csv\\')\\n    # all_dfs = { all_csv : pandas.read_csv(f\\'{basedir}/{all_csv}.csv\\') for all_csv in all_csvs}', metadata={'source': './DA_fm_analyze/fmanalyze/stats/quantiles.py'}),\n",
       " Document(page_content='import pandas as pd\\n\\n\\nsrt_keys = [\\'Acc\\', \\'Agi\\', \\'Ant\\', \\'Bal\\', \\'Bra\\', \\'Cmp\\', \\'Cnt\\', \\'Cro\\', \\'Dec\\', \\'Dri\\', \\'Fin\\', \\'Fir\\', \\'Fla\\',\\n            \\'Hea\\', \\'Jum\\', \\'Ldr\\', \\'Lon\\', \\'Mar\\', \\'OtB\\', \\'Pac\\', \\'Pas\\', \\'Pos\\', \\'Sta\\', \\'Str\\', \\'Tck\\', \\'Tea\\',\\n            \\'Tec\\', \\'Vis\\', \\'Wor\\']\\n\\n\\nATTRS_TO_ADD = {\\n    \"Aerial Presence\" : [\\'Jum\\'],\\n    \"Awareness\": [\\'Ant\\', \\'Tea\\', \\'Vis\\'],\\n    \"Clearing\": [\\'Hea\\'],\\n    \"Closing Down\" : [\\'Pos\\', \\'Cmp\\'],\\n    \"Control\": [\\'Fir\\', \\'Tec\\'],\\n    \"Creativity\": [\\'Ant\\', \\'Dec\\', \\'Fla\\'],\\n    \"Crossing\": [\\'Cro\\', \\'Pas\\', \\'Tec\\', \\'Cmp\\'],\\n    \"Decision Making\": [\\'Dec\\', \\'Tea\\'],\\n    \"Defensive Positioning\": [\\'Pos\\', \\'Cmp\\'],\\n    \"Dribbling\" : [\\'Dri\\', \\'Tec\\' ],\\n    \"Focus\": [\\'Cmp\\', \\'Cnt\\'],\\n    \"Endeavour\" : [\\'Agg\\', \\'Bra\\', \\'Det\\', \\'Wor\\'],\\n    \"Intelligence\" : [\\'Ant\\', \\'Dec\\', \\'Fla\\', \\'Tea\\', \\'Vis\\'],\\n    \"Marking\" : [\\'Mar\\' , \\'Cmp\\', \\'Pos\\'],\\n    \"Mobility\" : [\\'Acc\\', \\'Agi\\', \\'Bal\\', \\'Pac\\'],\\n    \"Movement\" : [\\'Ant\\', \\'Dec\\', \\'Tea\\'],\\n    \"Off The Ball\" : [\\'OtB\\', \\'Cmp\\'],\\n    \"Passing\" : [\\'Pas\\', \\'Tec\\', \\'Cmp\\'],\\n    \"Physical Presence\" : [\\'Str\\', \\'Bal\\'],\\n    \"Tackling\" : [\\'Tck\\', \\'Cmp\\'],\\n\\n}\\n\\ndef add_attr(sdf, tdf, new_col, cols):\\n    tdf[new_col] = sdf[cols].mean(axis=1)\\n\\ndef build_instrs(basedir, all_attrs_df):\\n    inst_df = pd.DataFrame()\\n    for attr, cols in ATTRS_TO_ADD.items():\\n        add_attr(all_attrs_df, inst_df, attr, cols)\\n    inst_df.to_csv(f\\'{basedir}/insts.csv\\', index=False)\\n    return inst_df', metadata={'source': './DA_fm_analyze/fmanalyze/attrs/instructions.py'}),\n",
       " Document(page_content='srt_keys = [\\'Acc\\', \\'Agi\\', \\'Ant\\', \\'Bal\\', \\'Bra\\', \\'Cmp\\', \\'Cnt\\', \\'Cro\\', \\'Dec\\', \\'Dri\\', \\'Fin\\', \\'Fir\\', \\'Fla\\',\\n            \\'Hea\\', \\'Jum\\', \\'Ldr\\', \\'Lon\\', \\'Mar\\', \\'OtB\\', \\'Pac\\', \\'Pas\\', \\'Pos\\', \\'Sta\\', \\'Str\\', \\'Tck\\', \\'Tea\\',\\n            \\'Tec\\', \\'Vis\\', \\'Wor\\']\\n\\ntecabi_keys = [ \"Clearing\", \"Control\", \"Crossing\", \"Dribbling\", \"Off The Ball\", \"Passing\", \"Shooting\", \"Tackling\"]\\nmenabi_keys = [ \"Awareness\", \"Closing\", \"Creativity\", \"Decision\", \"Defensive\", \"Endeavour\",\\n                \"Focus\", \"Intelligence\", \"Marking\", \"Movement\"]\\nphsyabi_keys = [ \"Aerial\", \"Mobility\", \"Physical\" ]\\n\\nATTRS_TO_ADD = {\\n    \"Aerial\" : [\\'Jum\\'],\\n    \"Awareness\": [\\'Ant\\', \\'Tea\\', \\'Vis\\'],\\n    \"Clearing\": [\\'Hea\\'],\\n    \"Closing\" : [\\'Pos\\', \\'Cmp\\'],\\n    \"Control\": [\\'Fir\\', \\'Tec\\'],\\n    \"Creativity\": [\\'Ant\\', \\'Dec\\', \\'Fla\\'],\\n    \"Crossing\": [\\'Cro\\', \\'Pas\\', \\'Tec\\'],\\n    \"Decision\": [\\'Dec\\', \\'Tea\\'],\\n    \"Defensive\": [\\'Pos\\', \\'Cmp\\'],\\n    \"Dribbling\" : [\\'Dri\\', \\'Tec\\' ],\\n    \"Focus\": [\\'Cmp\\', \\'Cnt\\'],\\n    \"Endeavour\" : [\\'Agg\\', \\'Bra\\', \\'Det\\', \\'Wor\\'],\\n    \"Intelligence\" : [\\'Ant\\', \\'Dec\\', \\'Fla\\', \\'Tea\\', \\'Vis\\'],\\n    \"Marking\" : [\\'Mar\\' , \\'Cmp\\', \\'Pos\\'],\\n    \"Mobility\" : [\\'Acc\\', \\'Agi\\', \\'Bal\\', \\'Pac\\'],\\n    \"Movement\" : [\\'Ant\\', \\'Dec\\', \\'Tea\\'],\\n    \"Off The Ball\" : [\\'OtB\\'],\\n    \"Passing\" : [\\'Pas\\', \\'Tec\\'],\\n    \"Physical\" : [\\'Str\\', \\'Bal\\'],\\n    \"Shooting\" : [\\'Fin\\', \\'Lon\\', \\'Tec\\'],\\n    \"Tackling\" : [\\'Tck\\'],\\n\\n}\\n\\ncolumns_to_round = list(ATTRS_TO_ADD.keys())\\n\\ndef create_abilities(basedir, df):\\n    new_df = df[[\\'Player\\', \\'UID\\']].copy()\\n    for attr, cols in ATTRS_TO_ADD.items():\\n        new_df[attr] = df[cols].mean(axis=1)\\n\\n\\n    new_df[columns_to_round] = new_df[columns_to_round].round(2)\\n    new_df.to_csv(f\\'{basedir}/abis.csv\\', index=False)\\n    return new_df\\n\\ndef split_abilities(df):\\n    columns = [\\'Player\\', \\'UID\\']\\n    if \"Position\" in df.columns:\\n        columns.append(\\'Position\\')\\n    tec_abis = df[columns + tecabi_keys].copy()\\n    men_abis = df[columns + menabi_keys].copy()\\n    phys_abis = df[columns + phsyabi_keys].copy()\\n\\n    return tec_abis, men_abis, phys_abis', metadata={'source': './DA_fm_analyze/fmanalyze/attrs/abilities.py'}),\n",
       " Document(page_content='GK_Attrs = [\"Aer\",\"Cmd\",\"Com\",\"Ecc\",\"Han\",\"Kic\",\"Pun\",\"1v1\",\"Ref\",\"TRO\",\"Thr\"]\\n\\ndef create_octs(basedir, df):\\n    # Create a new DataFrame with only the Player and UID columns\\n    new_df = df[[\\'Player\\', \\'UID\\']].copy()\\n    # Calculate the mean of the relevant attributes for each new attribute\\n    new_df.loc[:, \\'Defending\\'] = df[[\\'Tck\\', \\'Mar\\', \\'Pos\\']].mean(axis=1)\\n    new_df.loc[:, \\'Physical\\'] = df[[\\'Str\\', \\'Agi\\', \\'Bal\\', \\'Sta\\']].mean(axis=1)\\n    new_df.loc[:, \\'Speed\\'] = df[[\\'Pac\\', \\'Acc\\']].mean(axis=1)\\n    new_df.loc[:, \\'Vision\\'] = df[[\\'Pas\\', \\'Vis\\', \\'Fla\\']].mean(axis=1)\\n    new_df.loc[:, \\'Attacking\\'] = df[[\\'OtB\\', \\'Fin\\', \\'Cmp\\']].mean(axis=1)\\n    new_df.loc[:, \\'Technical\\'] = df[[\\'Tec\\', \\'Fir\\', \\'Dri\\']].mean(axis=1)\\n    new_df.loc[:, \\'Aerial\\'] = df[[\\'Jum\\', \\'Hea\\']].mean(axis=1)\\n    new_df.loc[:, \\'Mental\\'] = df[[\\'Dec\\', \\'Tea\\', \\'Ant\\', \\'Bra\\', \\'Det\\', \\'Cnt\\']].mean(axis=1)\\n    columns_to_round = [\\'Defending\\', \\'Physical\\', \\'Speed\\', \\'Vision\\', \\'Attacking\\', \\'Technical\\', \\'Aerial\\', \\'Mental\\']\\n    new_df[columns_to_round] = new_df[columns_to_round].round(2)\\n    new_df.to_csv(f\\'{basedir}/octs.csv\\', index=False)\\n    return new_df\\n\\ndef create_gk_octs(basedir, df):\\n    # Create a new DataFrame with only the Player and UID columns\\n    new_df = df[[\\'Player\\', \\'UID\\']].copy()\\n    # Calculate the mean of the relevant attributes for each new attribute\\n    new_df.loc[:, \\'Shot Stopping\\'] = df[[\\'1v1\\', \\'Ref\\']].mean(axis=1)\\n    new_df.loc[:, \\'Physical\\'] = df[[\\'Str\\', \\'Agi\\', \\'Bal\\', \\'Sta\\']].mean(axis=1)\\n    new_df.loc[:, \\'Speed\\'] = df[[\\'Pac\\', \\'Acc\\']].mean(axis=1)\\n    new_df.loc[:, \\'Mental\\'] = df[[\\'Dec\\', \\'Tea\\', \\'Ant\\', \\'Bra\\', \\'Det\\', \\'Cnt\\']].mean(axis=1)\\n    new_df.loc[:, \\'Communication\\'] = df[[\\'Cmd\\', \\'Com\\']].mean(axis=1)\\n    new_df.loc[:, \\'Eccentricity\\'] = df[[\\'Ecc\\']].mean(axis=1)\\n    new_df.loc[:, \\'Aerial\\'] = df[[\\'Aer\\', \\'Han\\']].mean(axis=1)\\n    new_df.loc[:, \\'Distribution\\'] = df[[\\'Kic\\', \\'Thr\\']].mean(axis=1)\\n    columns_to_round = [\\'Shot Stopping\\', \\'Physical\\', \\'Speed\\', \\'Mental\\', \\'Communication\\', \\'Eccentricity\\', \\'Aerial\\', \\'Distribution\\']\\n    new_df[columns_to_round] = new_df[columns_to_round].round(2)\\n    new_df.to_csv(f\\'{basedir}/gk_octs.csv\\', index=False)\\n    return new_df', metadata={'source': './DA_fm_analyze/fmanalyze/attrs/octs.py'}),\n",
       " Document(page_content='abbr_keys = [\\n    \\'Cro\\', \\'Dri\\', \\'Fin\\', \\'Fir\\', \\'Hea\\', \\'Lon\\', \\'Mar\\', \\'Pas\\',\\n    \\'Tck\\', \\'Tec\\',  \\'Ant\\', \\'Bra\\', \\'Cmp\\', \\'Cnt\\', \\'Dec\\',\\n    \\'Ldr\\', \\'OtB\\', \\'Pos\\', \\'Tea\\', \\'Vis\\', \\'Wor\\', \\'Acc\\', \\'Agi\\', \\'Bal\\', \\'Jum\\',\\n    \\'Pac\\', \\'Sta\\', \\'Str\\'\\n]\\n\\n\\nweights = {\\n    \\'GK\\': [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 6, 2, 6, 10, 2, 0, 5, 2, 1, 1, 6, 8, 2, 1, 3, 1, 4],\\n    \\'DRL\\': [2, 1, 1, 3, 2, 1, 3, 2, 4, 2, 3, 2, 2, 4, 7, 1, 1, 4, 2, 2, 2, 7, 6, 2, 2, 5, 6, 4],\\n    \\'DC\\': [1, 1, 1, 2, 5, 1, 8, 2, 5, 1, 5, 2, 2, 4, 10, 2, 1, 8, 1, 1, 2, 6, 6, 2, 6, 5, 3, 6],\\n    \\'WBRL\\': [3, 2, 1, 3, 1, 1, 2, 3, 3, 3, 3, 1, 2, 3, 5, 1, 2, 3, 2, 2, 2, 8, 5, 2, 1, 6, 7, 4],\\n    \\'DM\\': [1, 2, 2, 4, 1, 3, 3, 4, 7, 3, 5, 1, 2, 3, 8, 1, 1, 5, 2, 4, 4, 6, 6, 2, 1, 4, 4, 5],\\n    \\'MRL\\': [5, 3, 2, 4, 1, 2, 1, 3, 2, 4, 3, 1, 3, 2, 5, 1, 2, 1, 2, 3, 3, 8, 6, 2, 1, 6, 5, 3],\\n    \\'MC\\': [1, 2, 2, 6, 1, 3, 3, 6, 3, 4, 3, 1, 3, 2, 7, 1, 3, 3, 2, 6, 6, 6, 6, 2, 1, 5, 5, 4],\\n    \\'AMRL\\': [5, 5, 2, 5, 1, 2, 1, 2, 2, 4, 3, 1, 3, 2, 5, 1, 2, 1, 2, 3, 3, 10, 6, 2, 1, 10, 7, 3],\\n    \\'AMC\\' : [1, 3, 3, 5, 1, 3, 1, 4, 2, 5, 3, 1, 3, 2, 6, 1, 3, 2, 2, 6, 3, 9, 6, 2, 1, 7, 6 ,3],\\n    \\'ST\\' : [2, 5, 8, 6, 6, 2, 1, 2, 1, 4, 5, 1, 6, 2, 5, 1, 6, 2, 1, 2, 2, 10, 6, 2, 5, 7, 6, 6]\\n}\\n\\n\\ndef calculate_weighted_sum(basedir, df,  weights):\\n    new_df = df[[\\'Player\\', \\'UID\\']]\\n\\n    for role, weight_values in weights.items():\\n        sum_weights = sum(weight_values)\\n        weight_values_l = [x / sum_weights for x in weight_values]\\n        new_df[role] = df[abbr_keys].multiply(weight_values_l, axis=1).sum(axis=1)\\n    # Rounds all values to 2 decimals\\n    new_df = new_df.round(2)\\n    new_df.to_csv(f\\'{basedir}/wsums.csv\\', index=False)\\n\\n    return new_df\\n\\n\\n\\ndef calculate_weighted_sum_2(basedir, df,  weights):\\n    new_df = df[[\\'Player\\', \\'UID\\']]\\n\\n    dfc = df[abbr_keys]\\n\\n    for role, weight_values in weights.items():\\n        print(f\"Calculating weighted sum for role: {role}\")\\n        sum_weights = sum(weight_values)\\n        weight_values_l = [x / sum_weights for x in weight_values]\\n\\n        for idx, row in df.iterrows():\\n            new_df.at[idx, role] = 0\\n            print(f\"  For player {new_df.at[idx, \\'Player\\']}: \")\\n            idxstr = \"\"\\n            for col, weight_value_l in zip(dfc.columns, weight_values_l):\\n                idxstr += f\"\\'{col}\\': {weight_value_l} * \"\\n                idxstr += f\" {row[col]} + \"\\n                #idxstr += f\"weighted sum: {row[col] * weight_value}, \"\\n                new_df.at[idx, role] += row[col] * weight_value_l\\n            print(idxstr)\\n            print(f\"  Total weighted sum for index {idx}: {new_df.at[idx, role]}\\\\n\")\\n    new_df = new_df.round(2)\\n\\n    new_df.to_csv(f\\'{basedir}/wsums2.csv\\', index=False)\\n    return new_df', metadata={'source': './DA_fm_analyze/fmanalyze/attrs/positions.py'}),\n",
       " Document(page_content=\"player_roles_cm = {\\n    'DLP': ['Tec', 'Pas', 'Vis', 'Tec', 'Cmp', 'Ant', 'Dec', 'Pos', 'Tea', 'Cnt'],\\n    'Regista': ['Pas', 'Vis', 'Tec', 'Fir', 'Cro', 'Ant', 'Dec', 'OtB', 'Pos', 'Cnt'],\\n    'BWM': ['Tck', 'Agg', 'Wor', 'Sta', 'Pos', 'Tea', 'Str', 'Bra', 'Ant', 'Cnt'],\\n    'BBM': ['Sta', 'Wor', 'Fir', 'Pas', 'Tec', 'OtB', 'Pos', 'Ant', 'Dec', 'Tea'],\\n    'CM': ['Fir', 'Pas', 'Tec', 'Sta', 'Wor', 'Vis', 'Dec', 'Ant', 'Pos', 'Tea'],\\n    'AP': ['Fir', 'Pas', 'Vis', 'Tec', 'Cro', 'OtB', 'Ant', 'Dec', 'Cmp', 'Agi'],\\n    'Mezzala': ['Fir', 'Pas', 'Tec', 'Vis', 'OtB', 'Ant', 'Dec', 'Dri', 'Agi', 'Bal'],\\n    'Carrilero': ['Wor', 'Sta', 'Pos', 'Tea', 'Ant', 'Tck', 'Dec', 'Fir', 'Pas', 'Cnt']\\n}\\n\\nplayer_roles_st = {\\n    'AF': ['Acc', 'Pac', 'Fin', 'OtB', 'Ant', 'Cmp', 'Dri', 'Agi', 'Bal', 'Sta'],\\n    'CF': ['Fin', 'Fir', 'Hea', 'Pas', 'Tec', 'OtB', 'Ant', 'Vis', 'Wor', 'Str'],\\n    'DLF': ['Fir', 'Pas', 'Tec', 'Vis', 'OtB', 'Ant', 'Dec', 'Str', 'Bal', 'Cmp'],\\n    'F9': ['Fir', 'Pas', 'Tec', 'Vis', 'OtB', 'Ant', 'Dec', 'Dri', 'Bal', 'Cro'],\\n    'P': ['Fin', 'OtB', 'Ant', 'Acc', 'Pac', 'Cmp', 'Dri', 'Agi', 'Bal', 'Sta'],\\n    'PF': ['Wor', 'Sta', 'Agg', 'Str', 'Fin', 'OtB', 'Ant', 'Acc', 'Pac', 'Tea'],\\n    'TM': ['Hea', 'Str', 'Jum', 'Fir', 'Ant', 'Bal', 'OtB', 'Fin', 'Tea', 'Pas']\\n}\\n\\nplayer_roles_dm = {\\n    'DM': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Tea', 'Str', 'Dec'],\\n    'AM': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Tea', 'Str', 'Tea'],\\n    'BWM': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Tea', 'Str', 'Tea'],\\n    'HB': ['Tck', 'Agg', 'Wor', 'Sta', 'Pos', 'Tea', 'Str', 'Bra', 'Ant', 'Cnt'],\\n    'SV': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Fir', 'Pas', 'Cmp'],\\n    'RGA': ['Pas', 'Vis', 'Tec', 'Fir', 'Fla', 'Ant', 'Dec', 'OtB', 'Pos', 'Cnt']\\n}\\n\\nplayer_roles_cd = {\\n    'CD': ['Hea', 'Mar', 'Tck', 'Jum', 'Str', 'Pos', 'Ant', 'Cnt', 'Tea', 'Bra'],\\n    'BPD': ['Hea', 'Mar', 'Tck', 'Jum', 'Str', 'Pos', 'Ant', 'Cnt', 'Fir', 'Pas'],\\n    'NNCB': ['Hea', 'Mar', 'Tck', 'Jum', 'Str', 'Pos', 'Ant', 'Cnt', 'Bra', 'Agg'],\\n    'Lib': ['Fir', 'Pas', 'Tec', 'Vis', 'Ant', 'Pos', 'Cnt', 'Tck', 'Mar', 'OtB']\\n}\\n\\nplayer_roles_fb = {\\n    'FB': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Tea', 'Acc', 'Pac'],\\n    'WB': ['Tck', 'Mar', 'Pos', 'Sta', 'Wor', 'Cro', 'Acc', 'Pac', 'Dri', 'OtB'],\\n    'IWB': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Fir', 'Pas', 'Vis'],\\n    'CWB': ['Tck', 'Mar', 'Pos', 'Sta', 'Wor', 'Cro', 'Acc', 'Pac', 'Dri', 'OtB', 'Fir', 'Pas', 'Tec'],\\n    'NNFB': ['Tck', 'Mar', 'Pos', 'Ant', 'Cnt', 'Sta', 'Wor', 'Bra', 'Agg', 'Str']\\n}\\n\\nplayer_roles_wm = {\\n    'WM': ['Cro', 'Dri', 'Fir', 'Pas', 'Tck', 'Wor', 'Tea', 'Sta', 'Dec', 'OtB'],\\n    'W': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal'],\\n    'DW': ['Mar', 'Tck', 'Sta', 'Wor', 'Tea', 'Pos', 'Ant', 'Cnt', 'Cro', 'Pas'],\\n    'IW': ['Dri', 'Fir', 'Pas', 'Vis', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'IF': ['Dri', 'Fin', 'Fir', 'Pas', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'WG': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal', 'Hea', 'Jum']\\n}\\n\\nplayer_roles_amc = {\\n    'AMC': ['Fir', 'Pas', 'Tec', 'Vis', 'Dec', 'Wor', 'Sta', 'Tea', 'OtB', 'Cnt'],\\n    'AP': ['Fir', 'Pas', 'Tec', 'Vis', 'Fla', 'Dec', 'Wor', 'Sta', 'Tea', 'OtB'],\\n    'T': ['Fir', 'Pas', 'Tec', 'Vis', 'Dec', 'Wor', 'Sta', 'Tea', 'OtB', 'Cnt'],\\n    'SS': ['Dri', 'Fin', 'Fir', 'Pas', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'AM': ['Dri', 'Fir', 'Pas', 'Vis', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'EG': ['Dri', 'Fir', 'Pas', 'Vis', 'Tec', 'Sta', 'Wor', 'OtB', 'Fla', 'Bal']\\n}\", metadata={'source': './DA_fm_analyze/fmanalyze/attrs/roles.py'}),\n",
       " Document(page_content=\"player_roles_amlr = {\\n    'AML': ['Dri', 'Fir', 'Pas', 'Tec', 'Vis', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'AMR': ['Dri', 'Fir', 'Pas', 'Tec', 'Vis', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'IW_AML': ['Dri', 'Fir', 'Pas', 'Vis', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'IW_AMR': ['Dri', 'Fir', 'Pas', 'Vis', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'W_AML': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal'],\\n    'W_AMR': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal'],\\n    'IF_AML': ['Dri', 'Fin', 'Fir', 'Pas', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'IF_AMR': ['Dri', 'Fin', 'Fir', 'Pas', 'Tec', 'Sta', 'Wor', 'OtB', 'Ant', 'Dec'],\\n    'WG_AML': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal', 'Hea', 'Jum'],\\n    'WG_AMR': ['Cro', 'Dri', 'Acc', 'Pac', 'Sta', 'Wor', 'Tea', 'OtB', 'Fla', 'Bal', 'Hea', 'Jum']\\n}\\n\\n\\ndef create_roles(basedir, df, player_roles, output_file):\\n\\n    # Create an empty DataFrame with the same number of rows as the original DataFrame\\n    player_role_df = df[['Player', 'UID']]\\n\\n    # Calculate the average values for each player role\\n    # Calculate the average values for each player role\\n    for role, attributes in  player_roles.items():\\n        player_role_df[role] = df[attributes].mean(axis=1)\\n    # Round the values to two decimal places\\n    player_role_df = player_role_df.round(2)\\n\\n    player_role_df.to_csv(f'{basedir}/{output_file}', index=False)\\n    return player_role_df\\n\\n\\ndef create_all_roles(basedir, df):\\n    player_roles_data = {\\n        'cd': player_roles_cd,\\n        'fb': player_roles_fb,\\n        'dm': player_roles_dm,\\n        'cm': player_roles_cm,\\n        'wm': player_roles_wm,\\n        'amc': player_roles_amc,\\n        'amlr': player_roles_amlr,\\n        'st': player_roles_st\\n    }\\n    created_dfs = {}\\n    roles_to_create = ['cd', 'fb', 'dm', 'cm', 'wm', 'amc', 'amlr', 'st']\\n    for role in roles_to_create:\\n        created_dfs[role] = create_roles(basedir, df, player_roles_data[role], f'{role}.csv')\\n    return created_dfs\", metadata={'source': './DA_fm_analyze/fmanalyze/attrs/roles.py'}),\n",
       " Document(page_content='import pandas as pd\\nimport os\\n\\ntec_keys = [\\'Cro\\', \\'Dri\\', \\'Fin\\', \\'Fir\\', \\'Hea\\', \\'Lon\\', \\'Mar\\', \\'Pas\\', \\'Tck\\', \\'Tec\\']\\nmen_keys = [\\'Ant\\', \\'Bra\\', \\'Cmp\\', \\'Cnt\\', \\'Dec\\', \\'Ldr\\', \\'OtB\\', \\'Pos\\', \\'Tea\\', \\'Vis\\', \\'Wor\\']\\nphys_keys = [\\'Acc\\', \\'Agi\\', \\'Bal\\', \\'Jum\\', \\'Pac\\', \\'Sta\\', \\'Str\\']\\ngoalk_keys = [\\'Aer\\',\\'Cmd\\',\\'Com\\',\\'Ecc\\',\\'Han\\',\\'Kic\\',\\'Pun\\',\\'1v1\\',\\'Ref\\',\\'TRO\\',\\'Thr\\']\\n\\n\\ndef separate_in_tec_men_phys(df):\\n    columns = [\\'Player\\', \\'UID\\']\\n    if \"Position\" in df.columns:\\n        columns.append(\\'Position\\')\\n    tec_df = df[columns + tec_keys].copy()\\n    men_df = df[columns + men_keys].copy()\\n    phys_df = df[columns + phys_keys].copy()\\n    goalk_df = df[columns + goalk_keys].copy()\\n    return tec_df, men_df, phys_df, goalk_df\\n\\n\\ndef fill_color_df(df, color_df, quantile_dfs):\\n    pass\\ndef deal_with_fuzzy_attrs(i, field):\\n\\n    if i > 0 and \\'-\\' in field:\\n        first, lst = field.split(\\'-\\')\\n        if first.isdigit() and lst.isdigit():\\n            return round((float(first) + float(lst)) / 2, 2)\\n        else:\\n            return 0\\n    else:\\n        return field\\n\\ndef parse_attr_list(basedir, overwrite=True):\\n\\n    last_part = os.path.basename(basedir)\\n    attr_filename = f\\'{basedir}/{last_part}.rtf\\'\\n\\n\\n\\n    if os.path.isfile(attr_filename) and overwrite:\\n        with open(attr_filename, \\'r\\', encoding=\\'UTF-8\\') as file:\\n            raw_data = [line for line in file.readlines() if line.strip() ]\\n        print(raw_data)\\n\\n        columns = raw_data[0].strip().strip(\\'|\\').split(\\'|\\')\\n        columns = [x.strip() for x in columns]\\n        # Clean the data\\n        cleaned_data = []\\n        for line in raw_data:\\n            if not line or not line.strip() or not \\'Pick Player\\' in line:\\n                continue\\n            line = line.strip().replace(\" - Pick Player \", \"\").strip(\\'|\\')\\n\\n            cleaned_line = [x.strip() for x in line.split(\\'|\\')]\\n            cleaned_line = [deal_with_fuzzy_attrs(i, x) for i, x in enumerate(cleaned_line)]\\n            cleaned_data.append(cleaned_line)\\n        df = pd.DataFrame(cleaned_data, columns=columns)\\n        pos_df = df[[\"Player\", \"UID\", \"Position\"]].copy()\\n        df = df.drop(columns=[\"Position\"], axis=1)\\n        columns = df.columns\\n        for col in columns[1:]:\\n            df[col] = df[col].astype(float)\\n        df.to_csv(f\\'{basedir}/all_attrs.csv\\', encoding=\\'UTF-8\\', index=False)\\n        pos_df.to_csv(f\\'{basedir}/pos.csv\\', encoding=\\'UTF-8\\', index=False)\\n        return df, pos_df\\n    else:\\n        if os.path.isfile(f\\'{basedir}/all_attrs.csv\\'):\\n            df = pd.read_csv(f\\'{basedir}/all_attrs.csv\\')\\n            if os.path.isfile(f\\'{basedir}/pos.csv\\'):\\n                pos_df = pd.read_csv(f\\'{basedir}/pos.csv\\')\\n                return df, pos_df\\n            else:\\n                pos_df = df[[\"Player\", \"UID\", \"Position\"]].copy()\\n                pos_df.to_csv(f\\'{basedir}/pos.csv\\', encoding=\\'UTF-8\\', index=False)\\n                return df, pos_df\\n        else:\\n            print(f\\'Cannot find {attr_filename}\\')\\n            return None, None', metadata={'source': './DA_fm_analyze/fmanalyze/attrs/main_attrs.py'}),\n",
       " Document(page_content=\"POSITION_MAPPINGS = {\\n    'DCL': 'DC',\\n    'DCR': 'DC',\\n    'MCR': 'MC',\\n    'MCL': 'MC',\\n    'AMCR': 'AMC',\\n    'AMCL': 'AMC',\\n    'STC': 'STC',\\n    'STCL': 'STC',\\n    'STCR': 'STC',\\n    'DMR': 'DM',\\n    'DML': 'DM'\\n}\\n\\nPOSITION_SORT_ORDER = [\\n    'GK',\\n    'DR',\\n    'DCR',\\n    'DC',\\n    'DCL',\\n    'DL',\\n    'WBR',\\n    'DMR',\\n    'DM',\\n    'DML',\\n    'WBL',\\n    'MR',\\n    'MCR',\\n    'MC',\\n    'MCL',\\n    'ML',\\n    'AMR',\\n    'AMCR',\\n    'AMC',\\n    'AMCL',\\n    'AML',\\n    'STCR',\\n    'STC',\\n    'STCL'\\n]\\n\\ndef convert_position(position):\\n    if position in POSITION_MAPPINGS:\\n        return POSITION_MAPPINGS[position]\\n    else:\\n        return position\\n\\n\\ndef sort_positions(x):\\n    return POSITION_SORT_ORDER.index(x)\", metadata={'source': './DA_fm_analyze/fmanalyze/roles/utils.py'}),\n",
       " Document(page_content='import pandas as pd\\nimport os\\nimport io\\nfrom fmanalyze.roles.utils import sort_positions\\ndef parse_selection(basedir):\\n    if not os.path.isfile(f\\'{basedir}/csel.csv\\'):\\n        print(\"Cannot find csel.csv\")\\n        return None\\n\\n    with open(f\\'{basedir}/csel.csv\\', \\'r\\', encoding=\\'UTF-8\\') as file:\\n        raw_data = file.readlines()\\n    columns = raw_data[0].strip().strip(\\'|\\').split(\\'|\\')\\n    columns = [x.strip() for x in columns]\\n    cleaned_data = []\\n\\n    for index, line in enumerate(raw_data):\\n        if index == 0:\\n            continue\\n        if \"----\" in line:\\n            continue\\n\\n        line = line.strip().replace(\" - Pick Player \", \"\").strip(\\'|\\')\\n        cleaned_line = [x.strip() for x in line.split(\\'|\\')]\\n        if len(cleaned_line) < 2:\\n            continue\\n        if len(cleaned_line[0]) < 2:\\n            continue\\n        cleaned_data.append(cleaned_line)\\n\\n\\n    df = pd.DataFrame(cleaned_data, columns=columns)\\n    df[\"UID\"] = df[\"UID\"].astype(int)\\n    df.drop(columns=[\"Name\"], inplace=True, axis=1)\\n    return df\\n\\n\\ndef read_formation_for_select(formation_dir, formation_file =\\'full_squad.csv\\'):\\n    formation_full_file = os.path.join(formation_dir, formation_file)\\n    data = pd.read_csv(formation_full_file)\\n    fdata = data[[\\'Player\\', \\'UID\\']].drop_duplicates()\\n    fdata[\\'Surname\\'] = fdata[\\'Player\\'].apply(lambda name: name.split(\\' \\')[-1])\\n    fdata = fdata.sort_values(by=[\\'Surname\\']).drop(columns=[\\'Surname\\'])\\n    result_dict = {k: v for k, v in zip(fdata[\\'UID\\'], fdata[\\'Player\\'])}\\n    return result_dict\\n\\ndef read_selected_formation(formation_dir, formation_file):\\n    formation_full_file = os.path.join(formation_dir, formation_file)\\n    data = pd.read_csv(formation_full_file)\\n    formation_lists = (list(data[\\'Position\\']), list(data[\\'UID\\']))\\n    return formation_lists\\n\\ndef read_formation(formation_dir, formation_file = \\'full_squad.csv\\', full_squad = False, selected_role = None):\\n    formation_full_file = os.path.join(formation_dir, formation_file)\\n    with open(formation_full_file, \\'r\\', encoding=\\'UTF-8\\') as file:\\n        lines = file.readlines()\\n    if full_squad:\\n        filtered_lines = [line.replace(\\'#\\',\\'\\') for line in lines]\\n\\n    else:\\n        filtered_lines = [line for line in lines if not line.startswith(\\'#\\')]\\n\\n    data = pd.read_csv(io.StringIO(\\'\\'.join(filtered_lines)))\\n    if \\'Match\\' in data.columns:\\n#        data.drop(columns=[\\'Position\\'], inplace=True)\\n        data.rename(columns={\\'Match\\': \\'Position\\'}, inplace=True)\\n    data.sort_values(by=[\\'Position\\'], inplace=True, key=lambda x: x.map(sort_positions))\\n    if selected_role is not None:\\n        data = data[data[\\'Position\\'] == selected_role]\\n    return data\\n\\ndef save_formation(basedir, df):\\n    with open(os.path.join(basedir, \\'full_squad.csv\\'), \\'w\\', encoding=\\'UTF-8\\') as file:\\n        header = f\"{\\',\\'.join(df.columns)}\\\\n\"\\n        file.write(header)\\n\\n        # Write rows with \\'#\\' at the beginning\\n        for index, row in df.iterrows():\\n            formatted_row = f\"#{\\',\\'.join(row.astype(str))}\\\\n\"\\n            file.write(formatted_row)', metadata={'source': './DA_fm_analyze/fmanalyze/roles/formation.py'}),\n",
       " Document(page_content='import argparse\\nimport yaml\\nimport os\\nimport pandas as pd\\nfrom fmanalyze.attrs.positions import weights\\nfrom fmanalyze.roles.formation import save_formation\\n\\nCOL_ROLES = [\\'GK\\', \\'DR\\', \\'DC\\', \\'DL\\', \\'WBR\\', \\'DM\\', \\'WBL\\', \\'MR\\', \\'MC\\', \\'ML\\', \\'AMR\\', \\'AMC\\', \\'AML\\', \\'STC\\']\\n\\nPOS_MAP = {\\n    \\'GK\\': [\\'GK\\'],\\n    \\'DRL\\' : [\\'DR\\', \\'DL\\'],\\n    \\'DC\\' : [\\'DC\\'],\\n    \\'WBRL\\' : [\\'WBR\\', \\'WBL\\'],\\n    \\'DM\\' : [\\'DM\\'],\\n    \\'MRL\\' : [\\'MR\\', \\'ML\\'],\\n    \\'AMRL\\' : [\\'AMR\\', \\'AML\\'],\\n    \\'ST\\' : [\\'STC\\'],\\n    \\'MC\\' : [\\'MC\\'],\\n    \\'AMC\\' : [\\'AMC\\']\\n}\\n\\ndef expand_position(position, suffix):\\n    return [p.strip() + suffix for p in position.split(\\'/\\')]\\n\\ndef transform(input_str):\\n    comma_parts = [part.strip() for part in input_str.split(\\',\\')]\\n    result = []\\n\\n    for comma_part in comma_parts:\\n        if \\' \\' not in comma_part:\\n            result.append(comma_part)\\n            continue\\n        else:\\n            roles, sides = comma_part.split(\\' \\')\\n            for poss_side in [\\'R\\', \\'L\\', \\'C\\']:\\n                for role in roles.split(\\'/\\'):\\n                    if poss_side in sides:\\n                        result.append(role + poss_side)\\n    return result\\n\\n\\ndef extract_roles(basedir, pos_df=None):\\n    roles_df = pos_df[[\\'Player\\',\\'UID\\']].copy()\\n    # Add COL_ROLES to roles_df\\'s columns\\n    for col in COL_ROLES:\\n        roles_df[col] = None\\n\\n    # Iterate Rows in pos_df\\n    for index, row in pos_df.iterrows():\\n        position = row[\\'Position\\']\\n        poses = transform(position)\\n        for pos in poses:\\n            print(f\\'roles_df.loc[{index}, {pos}]\\')\\n            roles_df.loc[index, pos] = 1\\n    roles_df.fillna(0, inplace=True)\\n    for col in COL_ROLES:\\n        roles_df[col] = roles_df[col].astype(int)\\n\\n    roles_df.to_csv(os.path.join(basedir, \\'roles.csv\\'), index=False)\\n\\n\\ndef evaluate_positions(basedir, pos_df, wsums_df):\\n    exp_df = pos_df[[\\'Player\\',\\'UID\\']].copy()\\n    for index, row in wsums_df.iterrows():\\n        for col in wsums_df.columns:\\n            if col in POS_MAP:\\n                expanded_poses = POS_MAP[col]\\n                for expanded_pos in expanded_poses:\\n                    exp_df.loc[index, expanded_pos] = row[col]\\n    exp_df.to_csv(os.path.join(basedir, \\'eval_pos.csv\\'), index=False)\\n\\n\\ndef extract_match_roles(teams_dir):\\n    roles_df = pd.read_csv(os.path.join(teams_dir, \\'roles.csv\\'))\\n\\n    full_df = pd.DataFrame(columns=[\\'Match\\', \\'Player\\', \\'UID\\'])\\n    for col in COL_ROLES:\\n        # finds the rows where the role is not 0\\n        players_in_roles = roles_df[roles_df[col] != 0][[\\'Player\\', \\'UID\\']]\\n        players_in_roles.insert(0, \\'Match\\', col)\\n        players_in_roles[\\'Match\\'] = col\\n        full_df = pd.concat([full_df, players_in_roles], ignore_index=True)\\n\\n    save_formation(teams_dir, full_df)\\n    return full_df\\n        # print(f\"{col}: {len(ps)}\")', metadata={'source': './DA_fm_analyze/fmanalyze/roles/extract.py'}),\n",
       " Document(page_content='import pandas as pd\\n\\nfrom fmanalyze.roles.utils import convert_position\\nfrom fmanalyze.roles.extract import COL_ROLES\\n\\n\\ndef create_color_roles_dfs(league_dir, selected_role=None):\\n    color_roles_dfs = {}\\n    if selected_role is not None:\\n        col_loop_roles = [convert_position(selected_role)]\\n    else:\\n        col_loop_roles = COL_ROLES\\n    for role in col_loop_roles:\\n        quantile_abi_df_name, quantile_attr_df_name, quantile_octs_df_name, quantile_gk_octs_df_name = \\\\\\n            f\\'quantiles_{role}_abis\\', f\\'quantiles_{role}_attrs\\', f\\'quantiles_{role}_octs\\', f\\'quantiles_{role}_gk_octs\\'\\n        color_roles_dfs[quantile_abi_df_name] = pd.read_csv(f\\'{league_dir}/{quantile_abi_df_name}.csv\\')\\n        color_roles_dfs[quantile_attr_df_name] = pd.read_csv(f\\'{league_dir}/{quantile_attr_df_name}.csv\\')\\n        color_roles_dfs[quantile_octs_df_name] = pd.read_csv(f\\'{league_dir}/{quantile_octs_df_name}.csv\\')\\n        color_roles_dfs[quantile_gk_octs_df_name] = pd.read_csv(f\\'{league_dir}/{quantile_gk_octs_df_name}.csv\\')\\n\\n    return color_roles_dfs\\n\\n\\n\\n\\ndef fill_color_dfs(color_dfs, all_dfs, color_roles_dfs, selected_role=None):\\n    color_dfs.update({f\\'{name}_color\\': df.copy() for name, df in all_dfs.items()})\\n    for name, df in color_dfs.items():\\n        df.loc[:, ~df.columns.isin([\"Player\", \"UID\", \"Position\"])] = 0\\n    for name, df in all_dfs.items():\\n        if name in [\\'tec\\', \\'men\\', \\'phys\\', \\'goalk\\', \\'octs\\', \\'gk_octs\\', \\'tecabi\\', \\'menabi\\', \\'physabi\\']:\\n            print(\"Processing\", name)\\n            for item, row in df.iterrows():\\n                player, uid = row[\\'Player\\'], row[\\'UID\\']\\n                if not \\'Position\\' in row:\\n                    print(row)\\n                position = row[\\'Position\\']\\n                position = convert_position(position)\\n                if name in [\\'tec\\', \\'men\\', \\'phys\\', \\'goalk\\']:\\n                    ref_color_df = color_roles_dfs[f\\'quantiles_{position}_attrs\\']\\n                elif name in [\\'tecabi\\', \\'menabi\\', \\'physabi\\']:\\n                    ref_color_df = color_roles_dfs[f\\'quantiles_{position}_abis\\']\\n                elif name in [\\'octs\\']:\\n                    ref_color_df = color_roles_dfs[f\\'quantiles_{position}_octs\\']\\n                elif name in [\\'gk_octs\\']:\\n                      ref_color_df = color_roles_dfs[f\\'quantiles_{position}_gk_octs\\']\\n\\n                color_df = color_dfs[f\\'{name}_color\\']\\n\\n                for index, ref_color_row in ref_color_df.iterrows():\\n\\n                    col = ref_color_row[\\'COL\\']\\n                    if col in df.columns:\\n                        if row[col] < ref_color_row[\\'Q20\\']:\\n                            color_df.loc[(color_df[\\'UID\\'] == uid) & (color_df[\\'Position\\'] == position), col] = -2\\n                        if row[col] >= ref_color_row[\\'Q20\\'] and row[col] < ref_color_row[\\'Q40\\']:\\n                            color_df.loc[(color_df[\\'UID\\'] == uid) & (color_df[\\'Position\\'] == position), col] = -1\\n                        elif row[col] >= ref_color_row[\\'Q40\\'] and row[col] < ref_color_row[\\'Q60\\']:\\n                            color_df.loc[(color_df[\\'UID\\'] == uid) & (color_df[\\'Position\\'] == position), col] = 0\\n                        elif row[col] >= ref_color_row[\\'Q60\\'] and row[col] < ref_color_row[\\'Q80\\']:\\n                            color_df.loc[(color_df[\\'UID\\'] == uid) & (color_df[\\'Position\\'] == position), col] = 1\\n                        elif row[col] >= ref_color_row[\\'Q80\\']:\\n                            color_df.loc[(color_df[\\'UID\\'] == uid) & (color_df[\\'Position\\'] == position), col] = 2', metadata={'source': './DA_fm_analyze/fmanalyze/aggregate/color.py'}),\n",
       " Document(page_content='from fmanalyze.attrs.abilities import create_abilities\\nfrom fmanalyze.attrs.instructions import build_instrs\\nfrom fmanalyze.attrs.main_attrs import parse_attr_list\\nfrom fmanalyze.attrs.octs import create_octs, create_gk_octs\\nfrom fmanalyze.attrs.positions import calculate_weighted_sum, weights\\nfrom fmanalyze.attrs.roles import create_all_roles\\nfrom fmanalyze.roles.extract import evaluate_positions, extract_roles, extract_match_roles\\n\\n\\ndef create_dfs_for_basedir(basedir, overwrite=True):\\n    df, pos_df = parse_attr_list(basedir, overwrite=overwrite)\\n    octs_df = create_octs(basedir, df)\\n    gk_octs_df = create_gk_octs(basedir, df)\\n    all_roles_df = create_all_roles(basedir, df)\\n    wsums_df = calculate_weighted_sum(basedir, df, weights)\\n    evaluate_positions(basedir, pos_df, wsums_df)\\n    abis_df = create_abilities(basedir, df)\\n    instrs = build_instrs(basedir, df)\\n    extract_roles(basedir, pos_df)\\n    extract_match_roles(basedir)', metadata={'source': './DA_fm_analyze/fmanalyze/aggregate/main.py'}),\n",
       " Document(page_content=\"import pandas\\n\\nfrom fmanalyze.aggregate.color import create_color_roles_dfs, fill_color_dfs\\nfrom fmanalyze.attrs.abilities import split_abilities\\nfrom fmanalyze.attrs.main_attrs import separate_in_tec_men_phys\\nfrom fmanalyze.roles.formation import read_formation, read_formation_for_select\\n\\n\\ndef fill_all_dfs(all_dfs, basedir, formation_df=None, selected_role=None):\\n    all_octs = pandas.read_csv(f'{basedir}/octs.csv')\\n    all_gk_octs = pandas.read_csv(f'{basedir}/gk_octs.csv')\\n    all_attrs = pandas.read_csv(f'{basedir}/all_attrs.csv')\\n    all_abilities = pandas.read_csv(f'{basedir}/abis.csv')\\n    if formation_df is not None:\\n        all_attrs = form_merge(all_attrs, formation_df)\\n        all_abilities = form_merge(all_abilities, formation_df)\\n        all_octs = form_merge(all_octs, formation_df)\\n        all_gk_octs = form_merge(all_gk_octs, formation_df)\\n    if selected_role is not None:\\n        all_attrs = all_attrs[all_attrs['Position'] == selected_role]\\n        all_abilities = all_abilities[all_abilities['Position'] == selected_role]\\n        all_octs = all_octs[all_octs['Position'] == selected_role]\\n        all_gk_octs = all_gk_octs[all_gk_octs['Position'] == selected_role]\\n\\n    all_dfs['octs'] = all_octs\\n    all_dfs['gk_octs'] = all_gk_octs\\n    all_dfs['tec'], all_dfs['men'], all_dfs['phys'], all_dfs['goalk'] = separate_in_tec_men_phys(all_attrs)\\n    all_dfs['tecabi'], all_dfs['menabi'], all_dfs['physabi'] = split_abilities(all_abilities)\\n    for key, df in all_dfs.items():\\n        if 'Position' in df.columns:\\n            temp_column = df.pop('Position')\\n            df.insert(0, 'Position', temp_column)\\n\\n\\ndef form_merge(ref_df, formation_df):\\n    ref_df = formation_df.merge(ref_df, on='UID', how='inner')\\n    if 'Player_y' in ref_df.columns:\\n        ref_df =ref_df.drop(columns=['Player_y']).rename(columns={'Player_x': 'Player'})\\n    return ref_df\\n\\n\\n#        if 'gk' in key:\\n#            all_dfs[key] = df[df['Position'] == 'GK']\\n#        else:\\n#            all_dfs[key] = df[df['Position'] != 'GK']\\n\\n\\ndef create_full_squad_dfs(teamdir, quantilesdir, own_all_dfs, color_dfs, formation=None, selected_role=None):\\n    formation_file = formation if formation else 'full_squad.csv'\\n    formation_df = read_formation(teamdir, formation_file, full_squad=True, selected_role=selected_role)\\n    color_roles_dfs = create_color_roles_dfs(quantilesdir, selected_role=selected_role)\\n    fill_all_dfs(own_all_dfs, teamdir, formation_df, selected_role)\\n    fill_color_dfs(color_dfs, own_all_dfs, color_roles_dfs, selected_role)\\n\\n\\n\\ndef create_formation_dfs(teamdir, rivaldir, quantilesdir, formation_df, formation_rival_df, own_all_dfs, color_dfs, rival_all_dfs, rival_color_dfs):\\n\\n    color_roles_dfs = create_color_roles_dfs(quantilesdir)\\n    fill_all_dfs(own_all_dfs, teamdir, formation_df)\\n    fill_color_dfs(color_dfs, own_all_dfs, color_roles_dfs)\\n    if rivaldir is not None:\\n        fill_all_dfs(rival_all_dfs, rivaldir, formation_rival_df)\\n        fill_color_dfs(rival_color_dfs, rival_all_dfs, color_roles_dfs)\\n\\n\\ndef read_formations(teamdir, formation, rivaldir, rivalformation):\\n    formation_df, formation_rival_df = None, None\\n    if formation:\\n        formation_df = read_formation(teamdir, formation)\\n    else:\\n        formation_df = read_formation(teamdir, full_squad=True)\\n    if rivaldir is not None:\\n        if rivalformation is not None:\\n            formation_rival_df = read_formation(rivaldir, rivalformation)\\n        else:\\n            formation_rival_df = read_formation(rivaldir, full_squad=True)\\n    return formation_df, formation_rival_df\", metadata={'source': './DA_fm_analyze/fmanalyze/aggregate/collect.py'}),\n",
       " Document(page_content='import argparse\\nimport pandas\\nimport yaml\\nfrom fmanalyze.roles.extract import COL_ROLES\\nimport sys\\nfrom PyQt5.QtWidgets import QApplication, QTableWidget, QTableWidgetItem, QVBoxLayout, QWidget, QTabWidget\\nfrom PyQt5.QtGui import QColor, QStandardItemModel\\n\\n\\n# Define a custom style function\\ndef color_based_on_distribution(column):\\n    q1 = column.quantile(0.25)\\n    q2 = column.quantile(0.5)\\n    q3 = column.quantile(0.75)\\n\\n    color_mapper = []\\n\\n    for value in column:\\n        if value <= q1:\\n            color_mapper.append(QColor(\\'red\\'))\\n        elif q1 < value <= q2:\\n            color_mapper.append(QColor(\\'yellow\\'))\\n        elif q2 < value <= q3:\\n            color_mapper.append(QColor(\\'lightgreen\\'))\\n        else:\\n            color_mapper.append(QColor(\\'green\\'))\\n\\n    return color_mapper\\n\\n# Create a PyQt5 table widget from a pandas DataFrame with color-coded cells\\n\\ndef create_table_widget(df):\\n    table = QTableWidget()\\n    table.setRowCount(df.shape[0])\\n    table.setColumnCount(df.shape[1])\\n    table.setSortingEnabled(True)\\n    # Set a specific column to be pinned to the left\\n    #table.verticalHeader().setSectionResizeMode(QHeaderView.ResizeToContents)\\n\\n    for i, column in enumerate(df.columns):\\n        for j, value in enumerate(df[column]):\\n            item = QTableWidgetItem(str(value))\\n            # get type of columne\\n            if df[column].dtype == \\'int64\\' or df[column].dtype == \\'float64\\':\\n                item.setBackground(color_based_on_distribution(df[column])[j])\\n            table.setItem(j, i, item)\\n\\n    table.setHorizontalHeaderLabels(df.columns)\\n    return table\\n\\n# Create a PyQt5 application with tabs for each DataFrame\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\')\\n    args = parser.parse_args()\\n    if args.config == None:\\n        print(\"required argument --config <config>\")\\n        exit()\\n    else:\\n        with open(args.config, \\'r\\') as confhandle:\\n            config = yaml.safe_load(confhandle)\\n\\n\\n\\n    basedir = config[\"target_dir\"]\\n    all_csvs = [f\\'{role}_attrs\\' for role in COL_ROLES]\\n    all_dfs = {}\\n\\n    for csv in all_csvs:\\n        csv_df = pandas.read_csv(f\\'{basedir}/{csv}.csv\\')\\n        all_dfs[csv] = csv_df\\n\\n    app = QApplication(sys.argv)\\n\\n    window = QWidget()\\n    layout = QVBoxLayout(window)\\n    tab_widget = QTabWidget()\\n\\n    for name, df in all_dfs.items():\\n        model = QStandardItemModel()\\n\\n        table_widget = create_table_widget(df)\\n        tab_widget.addTab(table_widget, name)\\n\\n    layout.addWidget(tab_widget)\\n    window.show()\\n\\n    sys.exit(app.exec_())', metadata={'source': './DA_fm_analyze/test/qtgui.py'}),\n",
       " Document(page_content='import argparse\\nimport pandas\\nimport yaml\\nfrom fmanalyze.roles.extract import COL_ROLES\\nimport pandas as pd\\nimport dash\\nimport dash_table\\nimport dash_html_components as html\\nimport dash_core_components as dcc\\nfrom dash.dependencies import Input, Output\\n\\n# Create sample DataFrames\\ndata1 = {\\n    \\'category\\': [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\', \\'F\\', \\'G\\', \\'H\\'],\\n    \\'value\\': [10, 20, 30, 40, 50, 60, 70, 80]\\n}\\ndf1 = pd.DataFrame(data1)\\n\\ndata2 = {\\n    \\'category\\': [\\'I\\', \\'J\\', \\'K\\', \\'L\\', \\'M\\', \\'N\\', \\'O\\', \\'P\\'],\\n    \\'value\\': [15, 25, 35, 45, 55, 65, 75, 85]\\n}\\ndf2 = pd.DataFrame(data2)\\n\\ndataframes = {\\'DataFrame 1\\': df1, \\'DataFrame 2\\': df2}\\n\\n# Define a custom style function\\ndef color_based_on_distribution(column):\\n    q1 = column.quantile(0.25)\\n    q2 = column.quantile(0.5)\\n    q3 = column.quantile(0.75)\\n\\n    color_mapper = []\\n\\n    for value in column:\\n        if value <= q1:\\n            color_mapper.append(\\'red\\')\\n        elif q1 < value <= q2:\\n            color_mapper.append(\\'yellow\\')\\n        elif q2 < value <= q3:\\n            color_mapper.append(\\'lightgreen\\')\\n        else:\\n            color_mapper.append(\\'green\\')\\n\\n    return color_mapper\\n\\n@app.callback(Output(\\'tab-content\\', \\'children\\'), [Input(\\'tabs\\', \\'value\\')])\\ndef render_content(tab):\\n    df = dataframes[tab]\\n    return dash_table.DataTable(\\n        columns=[{\"name\": i, \"id\": i} for i in df.columns if i != \\'color\\'],\\n        data=df.to_dict(\\'records\\'),\\n        style_data_conditional=[\\n            {\\n                \\'if\\': {\\n                    \\'column_id\\': \\'value\\',\\n                    \\'row_index\\': i\\n                },\\n                \\'backgroundColor\\': df.loc[i, \\'color\\']\\n            } for i in range(df.shape[0])\\n        ]\\n    )\\n\\n\\nif __name__ == \"__main__\":\\n    parser = argparse.ArgumentParser()\\n\\n    parser.add_argument(\\'--config\\')\\n    args = parser.parse_args()\\n    if args.config == None:\\n        print(\"required argument --config <config>\")\\n        exit()\\n    else:\\n        with open(args.config, \\'r\\') as confhandle:\\n            config = yaml.safe_load(confhandle)\\n\\n\\n\\n    basedir = config[\"target_dir\"]\\n\\n\\n    all_csvs = [f\\'{role}_attrs\\' for role in COL_ROLES]\\n    all_dfs = {}\\n\\n    for csv in all_csvs:\\n        csv_df = pandas.read_csv(f\\'{basedir}/{csv}.csv\\')\\n        all_dfs[csv] = csv_df\\n\\n    # Create a Dash app\\n    app = dash.Dash(__name__)\\n\\n    app.layout = html.Div([\\n        dcc.Tabs(id=\\'tabs\\', value=\\'DataFrame 1\\', children=[\\n            dcc.Tab(label=name, value=name) for name in all_dfs.keys()\\n        ]),\\n        html.Div(id=\\'tab-content\\')\\n    ])', metadata={'source': './DA_fm_analyze/test/dashgui.py'}),\n",
       " Document(page_content='import dash\\nfrom dash import html\\nfrom dash import dcc\\nimport dash_table\\nimport pandas as pd\\n\\n# Create a sample DataFrame\\ndata = {\\'Column1\\': [1, 2, 3],\\n        \\'Column2\\': [4, 5, 6],\\n        \\'Column3\\': [7, 8, 9]}\\ndf = pd.DataFrame(data)\\n\\n# Initialize the Dash app\\napp = dash.Dash(__name__)\\n\\n# Define the layout of the app\\napp.layout = html.Div([\\n    dash_table.DataTable(\\n        id=\\'table\\',\\n        columns=[{\"name\": i, \"id\": i} for i in df.columns],\\n        data=df.to_dict(\\'records\\'),\\n        style_data_conditional=[\\n            # Color the first cell in the first row\\n            {\\n                \\'if\\': {\\'row_index\\': 0, \\'column_id\\': \\'Column1\\'},\\n                \\'backgroundColor\\': \\'red\\',\\n            },\\n            # Color the second cell in the second row\\n            {\\n                \\'if\\': {\\'row_index\\': 1, \\'column_id\\': \\'Column2\\'},\\n                \\'backgroundColor\\': \\'blue\\',\\n            },\\n            # Color the third cell in the third row\\n            {\\n                \\'if\\': {\\'row_index\\': 2, \\'column_id\\': \\'Column3\\'},\\n                \\'backgroundColor\\': \\'green\\',\\n            },\\n        ]\\n    ),\\n])\\n\\nif __name__ == \\'__main__\\':\\n    app.run_server(debug=True)', metadata={'source': './DA_fm_analyze/test/dashcolor.py'}),\n",
       " Document(page_content='import pandas as pd\\nimport dash\\nimport dash_table\\nimport dash_html_components as html\\nimport dash_core_components as dcc\\nfrom dash.dependencies import Input, Output\\n\\n# Create sample DataFrames\\ndata1 = {\\n    \\'category\\': [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\', \\'F\\', \\'G\\', \\'H\\'],\\n    \\'value\\': [10, 20, 30, 40, 50, 60, 70, 80]\\n}\\ndf1 = pd.DataFrame(data1)\\n\\ndata2 = {\\n    \\'category\\': [\\'I\\', \\'J\\', \\'K\\', \\'L\\', \\'M\\', \\'N\\', \\'O\\', \\'P\\'],\\n    \\'value\\': [15, 25, 35, 45, 55, 65, 75, 85]\\n}\\ndf2 = pd.DataFrame(data2)\\n\\ndataframes = {\\'DataFrame 1\\': df1, \\'DataFrame 2\\': df2}\\n\\n# Define a custom style function\\ndef color_based_on_distribution(column):\\n    q1 = column.quantile(0.25)\\n    q2 = column.quantile(0.5)\\n    q3 = column.quantile(0.75)\\n\\n    color_mapper = []\\n\\n    for value in column:\\n        if value <= q1:\\n            color_mapper.append(\\'red\\')\\n        elif q1 < value <= q2:\\n            color_mapper.append(\\'yellow\\')\\n        elif q2 < value <= q3:\\n            color_mapper.append(\\'lightgreen\\')\\n        else:\\n            color_mapper.append(\\'green\\')\\n\\n    return color_mapper\\n\\n# Apply the custom style to the DataFrames\\nfor name, df in dataframes.items():\\n    df[\\'color\\'] = color_based_on_distribution(df[\\'value\\'])\\n\\n# Create a Dash app\\napp = dash.Dash(__name__)\\n\\napp.layout = html.Div([\\n    dcc.Tabs(id=\\'tabs\\', value=\\'DataFrame 1\\', children=[\\n        dcc.Tab(label=name, value=name) for name in dataframes.keys()\\n    ]),\\n    html.Div(id=\\'tab-content\\')\\n])\\n\\n@app.callback(Output(\\'tab-content\\', \\'children\\'), [Input(\\'tabs\\', \\'value\\')])\\ndef render_content(tab):\\n    df = dataframes[tab]\\n    return dash_table.DataTable(\\n        columns=[{\"name\": i, \"id\": i} for i in df.columns if i != \\'color\\'],\\n        data=df.to_dict(\\'records\\'),\\n        style_data_conditional=[\\n            {\\n                \\'if\\': {\\n                    \\'column_id\\': \\'value\\',\\n                    \\'row_index\\': i\\n                },\\n                \\'backgroundColor\\': df.loc[i, \\'color\\']\\n            } for i in range(df.shape[0])\\n        ]\\n    )\\n\\nif __name__ == \\'__main__\\':\\n    app.run_server(debug=True)', metadata={'source': './DA_fm_analyze/test/test_dash.py'}),\n",
       " Document(page_content='import dash\\nimport dash_core_components as dcc\\nimport dash_html_components as html\\nfrom dash.dependencies import Input, Output, State\\nimport pandas as pd\\nimport dash_table\\n\\napp = dash.Dash(__name__)\\n\\n# Sample DataFrame\\ndata = {\"Category\": [\"Category1\", \"Category2\", \"Category3\"]}\\ndf = pd.DataFrame(data)\\n\\n# Create options for the combobox using DataFrame column data\\ndata_options = [{\"label\": category, \"value\": category} for category in df[\"Category\"]]\\n\\n# Define the layout of the app\\napp.layout = html.Div([\\n    html.H2(\"Select Categories\"),\\n    html.Div([\\n        dcc.Dropdown(id=\"combo1\", options=data_options),\\n        dcc.Dropdown(id=\"combo2\", options=data_options),\\n        html.Button(\"Submit\", id=\"submit-button\"),\\n    ]),\\n    html.Hr(),\\n    dcc.Tabs(id=\"tabs\"),\\n])\\n\\n@app.callback(\\n    Output(\"tabs\", \"children\"),\\n    [Input(\"submit-button\", \"n_clicks\")],\\n    [State(\"combo1\", \"value\"), State(\"combo2\", \"value\")],\\n)\\ndef generate_tabs(n_clicks, *args):\\n    if n_clicks is None:\\n        return []\\n\\n    # Access the selected values\\n    combo1_val, combo2_val = args\\n\\n    # Generate DataFrames based on the selected options\\n    data = {\"Column1\": [1, 2, 3], \"Column2\": [4, 5, 6], \"Column3\": [7, 8, 9]}\\n    df1 = pd.DataFrame(data)\\n    df2 = df1 * 2\\n\\n    # Create a list of tables to display in separate tabs\\n    tables = []\\n    for i, df in enumerate([df1, df2]):\\n        table = dash_table.DataTable(\\n            id=f\"table-{i}\",\\n            columns=[{\"name\": col, \"id\": col} for col in df.columns],\\n            data=df.to_dict(\"records\"),\\n        )\\n        tables.append(dcc.Tab(label=f\"Table {i+1}\", value=f\"tab-{i}\", children=[table]))\\n\\n    return tables\\n\\nif __name__ == \"__main__\":\\n    app.run_server(debug=True)', metadata={'source': './DA_fm_analyze/test/dash_multiple.py'}),\n",
       " Document(page_content=\"import sys\\nimport pandas as pd\\nfrom PyQt5.QtWidgets import QApplication, QTableWidget, QTableWidgetItem, QVBoxLayout, QWidget, QTabWidget\\nfrom PyQt5.QtGui import QColor\\n\\n# Create sample DataFrames\\ndata1 = {\\n    'category': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],\\n    'value': [10, 20, 30, 40, 50, 60, 70, 80]\\n}\\ndf1 = pd.DataFrame(data1)\\n\\ndata2 = {\\n    'category': ['I', 'J', 'K', 'L', 'M', 'N', 'O', 'P'],\\n    'value': [15, 25, 35, 45, 55, 65, 75, 85]\\n}\\ndf2 = pd.DataFrame(data2)\\n\\ndataframes = {'DataFrame 1': df1, 'DataFrame 2': df2}\\n\\n# Define a custom style function\\ndef color_based_on_distribution(column):\\n    q1 = column.quantile(0.25)\\n    q2 = column.quantile(0.5)\\n    q3 = column.quantile(0.75)\\n\\n    color_mapper = []\\n\\n    for value in column:\\n        if value <= q1:\\n            color_mapper.append(QColor('red'))\\n        elif q1 < value <= q2:\\n            color_mapper.append(QColor('yellow'))\\n        elif q2 < value <= q3:\\n            color_mapper.append(QColor('lightgreen'))\\n        else:\\n            color_mapper.append(QColor('green'))\\n\\n    return color_mapper\\n\\n# Create a PyQt5 table widget from a pandas DataFrame with color-coded cells\\ndef create_table_widget(df):\\n    table = QTableWidget()\\n    table.setRowCount(df.shape[0])\\n    table.setColumnCount(df.shape[1])\\n\\n    for i, column in enumerate(df.columns):\\n        for j, value in enumerate(df[column]):\\n            item = QTableWidgetItem(str(value))\\n            if column == 'value':\\n                item.setBackground(color_based_on_distribution(df[column])[j])\\n            table.setItem(j, i, item)\\n\\n    table.setHorizontalHeaderLabels(df.columns)\\n    return table\\n\\n# Create a PyQt5 application with tabs for each DataFrame\\napp = QApplication(sys.argv)\\n\\nwindow = QWidget()\\nlayout = QVBoxLayout(window)\\n\\ntab_widget = QTabWidget()\\n\\nfor name, df in dataframes.items():\\n    table_widget = create_table_widget(df)\\n    tab_widget.addTab(table_widget, name)\\n\\nlayout.addWidget(tab_widget)\\nwindow.show()\\n\\nsys.exit(app.exec_())\", metadata={'source': './DA_fm_analyze/test/test_pyqt5.py'}),\n",
       " Document(page_content=\"import dash\\nimport dash_core_components as dcc\\nimport dash_html_components as html\\nimport pandas as pd\\n\\napp = dash.Dash(__name__)\\n\\napp.layout = html.Div([\\n    dcc.Location(id='url', refresh=False),\\n    html.H1('Form Example'),\\n    html.Div([\\n        html.Label('Combobox 1'),\\n        dcc.Dropdown(\\n            id='combobox-1',\\n            options=[{'label': 'Option 1', 'value': 'option-1'},\\n                     {'label': 'Option 2', 'value': 'option-2'},\\n                     {'label': 'Option 3', 'value': 'option-3'}],\\n            value='option-1'\\n        ),\\n        html.Label('Combobox 2'),\\n        dcc.Dropdown(\\n            id='combobox-2',\\n            options=[{'label': 'Option A', 'value': 'option-a'},\\n                     {'label': 'Option B', 'value': 'option-b'},\\n                     {'label': 'Option C', 'value': 'option-c'}],\\n            value='option-a'\\n        ),\\n        html.Button('Submit', id='submit-button', n_clicks=0)\\n    ]),\\n    html.Div(id='tabs-content')\\n])\\n\\n@app.callback(\\n    dash.dependencies.Output('tabs-content', 'children'),\\n    [dash.dependencies.Input('submit-button', 'n_clicks')],\\n    [dash.dependencies.State('combobox-1', 'value'),\\n     dash.dependencies.State('combobox-2', 'value')])\\ndef update_output(n_clicks, combobox_1_value, combobox_2_value):\\n    if n_clicks > 0:\\n        # Generate the dataframes based on the input from the form\\n        df_1 = pd.DataFrame({'Column 1': [combobox_1_value, combobox_1_value],\\n                             'Column 2': [combobox_2_value, combobox_2_value]})\\n        df_2 = pd.DataFrame({'Column A': [combobox_1_value, combobox_1_value],\\n                             'Column B': [combobox_2_value, combobox_2_value]})\\n\\n        # Create a dcc.Tab component for each dataframe\\n        tabs = [dcc.Tab(label='Dataframe 1', children=[\\n                        dcc.Graph(figure={\\n                            'data': [{\\n                                'x': df_1['Column 1'],\\n                                'y': df_1['Column 2'],\\n                                'type': 'bar'\\n                            }]\\n                        })\\n                    ]),\\n                dcc.Tab(label='Dataframe 2', children=[\\n                    dcc.Graph(figure={\\n                        'data': [{\\n                            'x': df_2['Column A'],\\n                            'y': df_2['Column B'],\\n                            'type': 'scatter'\\n                        }]\\n                    })\\n                ])]\\n\\n        # Return the list of dcc.Tab components as the children of the dcc.Tabs component\\n        return dcc.Tabs(children=tabs)\\n\\nif __name__ == '__main__':\\n    app.run_server(debug=True)\", metadata={'source': './DA_fm_analyze/test/dashcategory.py'}),\n",
       " Document(page_content='<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <title>Main Page</title>\\n    <style>\\n        body {\\n            font-family: Arial, sans-serif;\\n            text-align: center;\\n            background-color: #f2f2f2;\\n        }\\n        h1 {\\n            font-size: 2.5em;\\n            color: #4CAF50;\\n        }\\n        .container {\\n            display: flex;\\n            justify-content: center;\\n            align-items: center;\\n            height: 100vh;\\n        }\\n        .buttons {\\n            display: flex;\\n            justify-content: space-around;\\n        }\\n        a {\\n            display: inline-block;\\n            background-color: #4CAF50;\\n            color: white;\\n            font-size: 1.5em;\\n            padding: 10px 20px;\\n            text-decoration: none;\\n            border-radius: 5px;\\n            margin: 10px;\\n        }\\n        a:hover {\\n            background-color: #45a049;\\n        }\\n    </style>\\n</head>\\n<body>\\n    <div class=\"container\">\\n        <div>\\n            <h1>Welcome to the Main Page</h1>\\n            <div class=\"buttons\">\\n                <a href=\"/formations\">FORMATIONS</a>\\n                <a href=\"/config\">CONFIG</a>\\n            </div>\\n        </div>\\n    </div>\\n</body>\\n</html>', metadata={'source': './DA_fm_analyze/templates/formation_index.html'}),\n",
       " Document(page_content='<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <title>Main Page</title>\\n    <style>\\n        body {\\n            font-family: Arial, sans-serif;\\n            text-align: center;\\n            background-color: #f2f2f2;\\n        }\\n        h1 {\\n            font-size: 2.5em;\\n            color: #4CAF50;\\n        }\\n        .container {\\n            display: flex;\\n            justify-content: center;\\n            align-items: center;\\n            height: 100vh;\\n        }\\n        .buttons {\\n            display: flex;\\n            justify-content: space-around;\\n        }\\n        a {\\n            display: inline-block;\\n            background-color: #4CAF50;\\n            color: white;\\n            font-size: 1.5em;\\n            padding: 10px 20px;\\n            text-decoration: none;\\n            border-radius: 5px;\\n            margin: 10px;\\n        }\\n        a:hover {\\n            background-color: #45a049;\\n        }\\n    </style>\\n</head>\\n<body>\\n    <div class=\"container\">\\n        <div>\\n            <h1>Welcome to the Main Page</h1>\\n            <div class=\"buttons\">\\n                <a href=\"/squads\">SQUADS</a>\\n                <a href=\"/config\">CONFIG</a>\\n            </div>\\n        </div>\\n    </div>\\n</body>\\n</html>', metadata={'source': './DA_fm_analyze/templates/squad_index.html'}),\n",
       " Document(page_content='basedir: \"/Users/diego/projects/DA_fm_data/laliga/\"\\nteam: \"rsana\"\\nformation: \"formation_barca.csv\"\\nrival: \"barc\"\\nrival_formation: \"formation_rsana.csv\"', metadata={'source': './DA_fm_analyze/teams/rsana.yaml'}),\n",
       " Document(page_content='.row {\\n    display: flex;\\n    justify-content: center;\\n    margin-bottom: 15px;\\n}\\n\\n.six.columns {\\n    flex-basis: 50%;\\n    padding-left: 10px;\\n    padding-right: 10px;\\n}', metadata={'source': './DA_fm_analyze/static/cb.css'}),\n",
       " Document(page_content='source_dir: \"/Users/diego/OneDrive/Nowy folder/Documents/Sports Interactive/Football Manager 2021/2proa\"\\ntarget_dir: \"/Users/diego/projects/DA_fm_data/2proa\"', metadata={'source': './DA_fm_analyze/leagues/2proa.yaml'}),\n",
       " Document(page_content='source_dir: \"/Users/diego/OneDrive/Nowy folder/Documents/Sports Interactive/Football Manager 2021/seriea\"\\ntarget_dir: \"/Users/diego/projects/DA_fm_data/seriea/\"', metadata={'source': './DA_fm_analyze/leagues/seriea.yaml'}),\n",
       " Document(page_content='source_dir: \"/Users/diego/OneDrive/Nowy folder/Documents/Sports Interactive/Football Manager 2021/2Db2\"\\ntarget_dir: \"/Users/diego/projects/DA_fm_data/2Db2\"', metadata={'source': './DA_fm_analyze/leagues/2Db2.yaml'}),\n",
       " Document(page_content='source_dir: \"/Users/diego/OneDrive/Nowy folder/Documents/Sports Interactive/Football Manager 2021/laliga\"\\ntarget_dir: \"/Users/diego/projects/DA_fm_data/laliga/\"\\nbasedir: \"/Users/diego/projects/DA_fm_data/laliga/\"', metadata={'source': './DA_fm_analyze/leagues/laliga.yaml'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9739b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1043, which is longer than the specified 1000\n",
      "Created a chunk of size 1133, which is longer than the specified 1000\n",
      "Created a chunk of size 1016, which is longer than the specified 1000\n",
      "Created a chunk of size 1020, which is longer than the specified 1000\n",
      "Created a chunk of size 1540, which is longer than the specified 1000\n",
      "Created a chunk of size 1202, which is longer than the specified 1000\n",
      "Created a chunk of size 1039, which is longer than the specified 1000\n",
      "Created a chunk of size 1252, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3416d77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8ab5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/diegoamicabile/da-fm-analyze\n",
      "hub://diegoamicabile/da-fm-analyze loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00\n",
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://diegoamicabile/da-fm-analyze', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (162, 1536)  float32   None   \n",
      "    ids      text     (162, 1)      str     None   \n",
      " metadata    json     (162, 1)      str     None   \n",
      "   text      text     (162, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a74c6a30-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6af8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6b34-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6b52-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6b70-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6b84-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ba2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6bc0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6bd4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6bf2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c06-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c24-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c38-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c56-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c6a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c88-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6c9c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6cba-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6cce-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6cec-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d00-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d1e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d32-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d50-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d64-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d82-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6d96-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6db4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6dc8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ddc-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6dfa-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e0e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e2c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e40-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e5e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e72-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6e90-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ea4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ec2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ed6-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6eea-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f08-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f26-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f3a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f4e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f6c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f80-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6f9e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6fb2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6fd0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6fe4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c6ff8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7016-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c702a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7048-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c70ac-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c70ca-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c70e8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c70fc-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c711a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c712e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c714c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7160-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7174-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7192-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c71b0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c71c4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c71d8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c71f6-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c720a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7228-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c723c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c725a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c726e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7282-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c72a0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c72b4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c72d2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c72f0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7304-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7322-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7336-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c734a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7368-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c737c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c739a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c73ae-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c73c2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c73e0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c73f4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7412-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7426-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7444-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7458-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7476-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c748a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c74a8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c74bc-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c74da-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c74f8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7516-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c752a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c753e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c755c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7570-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c758e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c75a2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c75c0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c75d4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c75f2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7606-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7624-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7638-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7656-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c766a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7688-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c769c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c76ba-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c76ce-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c76ec-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7700-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c771e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7732-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7750-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7764-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7782-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7796-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c77b4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c77c8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c77e6-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c77fa-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7818-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c782c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c784a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c785e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c787c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7890-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c78ae-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c78c2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c78e0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7908-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7926-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c793a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7958-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c796c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c798a-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c799e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c79bc-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c79d0-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c79ee-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a0c-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a20-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a3e-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a52-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a70-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7a84-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7aa2-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7ab6-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7ad4-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7ae8-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7b06-f29e-11ed-86b4-4ccc6ae1c359',\n",
       " 'a74c7b1a-f29e-11ed-86b4-4ccc6ae1c359']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = \"diegoamicabile\" # replace with your username from app.activeloop.ai\n",
    "db = DeepLake(dataset_path=f\"hub://{username}/da-fm-analyze\", embedding_function=embeddings, public=True) #dataset would be publicly available\n",
    "db.add_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13bcd38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/diegoamicabile/da-fm-analyze\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://diegoamicabile/da-fm-analyze loaded successfully.\n",
      "\n",
      "Deep Lake Dataset in hub://diegoamicabile/da-fm-analyze already exists, loading from the storage\n",
      "Dataset(path='hub://diegoamicabile/da-fm-analyze', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (162, 1536)  float32   None   \n",
      "    ids      text     (162, 1)      str     None   \n",
      " metadata    json     (162, 1)      str     None   \n",
      "   text      text     (162, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " \r",
      "\r",
      " \r"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path=\"hub://diegoamicabile/da-fm-analyze\", read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d702731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['fetch_k'] = 100\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39872396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-4') # switch to 'gpt-4'\n",
    "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de34fa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Can you describe what this repository does? \n",
      "\n",
      "**Answer**: I cannot provide a specific description of what this repository does, as the provided information does not include any details about the content or purpose of the code within the repository. The repository is named \"DA_fm_analyze\", but there is no description available to give more context. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Can you describe what this repository does?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f165af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Can you list the modules and the files in this repository ? \n",
      "\n",
      "**Answer**: I'm sorry, but the provided information does not contain a list of modules or files in the repository. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Can you list the modules and the files in this repository ?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb5286",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Can you list the modules and the files in this repository ?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
